================================================================================
GCP COST BILLING PIPELINE - INTEGRATION SUMMARY
================================================================================

Project: Convergence Data Pipeline
Date: November 18, 2025
Status: READY FOR TESTING

================================================================================
OVERVIEW
================================================================================

The GCP Cost Billing Pipeline is a production-ready system for ingesting
Google Cloud Platform billing data into BigQuery. It supports:

✅ Multi-tenant isolation
✅ Template-based configuration
✅ Async/parallel execution
✅ Comprehensive metadata logging
✅ Rate limiting & quota management
✅ Email notifications on failure
✅ Data partitioning & clustering
✅ REST API management

================================================================================
QUICK START (5 MINUTES)
================================================================================

1. START APPLICATION
   cd /Users/gurukallam/projects/cloudact-meta-data-store/cloudact-backend-systems/convergence-data-pipeline
   python -m uvicorn src.app.main:app --host 0.0.0.0 --port 8080 --reload

2. TRIGGER PIPELINE (in another terminal)
   curl -X POST http://localhost:8080/api/v1/pipelines/run/acme1281/gcp/cost/cost_billing \
     -H "Content-Type: application/json" \
     -d '{"date": "2024-11-01", "trigger_by": "quick_test"}'

3. MONITOR EXECUTION
   Copy pipeline_logging_id from response above, then:
   curl http://localhost:8080/api/v1/pipelines/runs/{pipeline_logging_id}

4. VERIFY IN BIGQUERY
   SELECT * FROM `gac-prod-471220.acme1281.x_meta_pipeline_runs`
   ORDER BY start_time DESC LIMIT 1

================================================================================
API ENDPOINTS
================================================================================

TEMPLATE-BASED PIPELINE EXECUTION (Recommended)
  POST /api/v1/pipelines/run/{tenant_id}/{provider}/{domain}/{template_name}
  
  Example:
  POST /api/v1/pipelines/run/docker_customer_3434x4/gcp/cost/cost_billing
  
  Payload:
  {
    "date": "2024-11-01",
    "trigger_by": "docker_test"
  }
  
  Response:
  {
    "pipeline_logging_id": "uuid-1234-5678-9abc",
    "pipeline_id": "docker_customer_3434x4-gcp-cost-billing",
    "tenant_id": "docker_customer_3434x4",
    "status": "PENDING",
    "message": "Pipeline triggered successfully..."
  }

MONITOR PIPELINE
  GET /api/v1/pipelines/runs/{pipeline_logging_id}
  
  Response:
  {
    "pipeline_logging_id": "uuid-1234-5678-9abc",
    "pipeline_id": "docker_customer_3434x4-gcp-cost-billing",
    "status": "RUNNING|COMPLETED|FAILED",
    "duration_ms": 45000,
    "start_time": "2025-11-18T16:08:00Z",
    "end_time": "2025-11-18T16:08:45Z"
  }

LIST PIPELINES
  GET /api/v1/pipelines/runs?limit=5&status=COMPLETED
  
ONBOARD CUSTOMER (if needed)
  POST /api/v1/tenants/onboard
  
  Payload:
  {
    "tenant_id": "docker_customer_3434x4",
    "company_name": "Docker Test Customer",
    "admin_email": "admin@docker-test.com",
    "subscription_plan": "starter"
  }

HEALTH CHECK
  GET /health
  GET /health/live
  GET /health/ready

================================================================================
FILE STRUCTURE
================================================================================

KEY FILES:

Configuration:
  configs/gcp/cost/cost_billing.yml
    - Pipeline template with dynamic variable substitution
    - Defines extraction query, destination table, partitioning
    - Email notification configuration

API Routes:
  src/app/routers/pipelines.py
    - Pipeline execution endpoints (templated)
    - Pipeline monitoring (GET status)
    - List recent runs
    - Deprecated pipeline triggers (backward compatibility)

  src/app/routers/tenants.py
    - Tenant onboarding (creates dataset, API keys, metadata tables)
    - Quota initialization

  src/app/routers/tenant_management.py
    - Customer profile management
    - Subscription management
    - API key creation/revocation
    - Cloud credentials storage

Pipeline Execution:
  src/core/pipeline/async_executor.py
    - Async pipeline execution
    - Step-by-step metadata logging
    - Error handling & notifications

  src/core/pipeline/template_resolver.py
    - Template file loading
    - Variable substitution ({tenant_id}, {date}, etc.)

Metadata Logging:
  src/core/metadata/logger.py
    - Batch logging to BigQuery
    - Pipeline run tracking
    - Step execution logging

Authentication:
  src/app/dependencies/auth.py
    - API key verification
    - Tenant context extraction
    - Rate limiting

Test Files:
  test_docker_customer_billing_pipeline.py
    - Full end-to-end test (onboarding + pipeline execution)
    - Infrastructure verification
    - Metadata validation

  test_pipeline_simple.py
    - Simplified pipeline test (no onboarding)
    - Works with existing tenants

Documentation:
  PIPELINE_EXECUTION_REPORT.md
    - Detailed architecture & troubleshooting
    - BigQuery queries
    - Full API reference

  GCP_COST_BILLING_PIPELINE_QUICKSTART.md
    - Step-by-step execution guide
    - Curl examples
    - Python examples

================================================================================
PIPELINE EXECUTION FLOW
================================================================================

1. API Request Received
   POST /api/v1/pipelines/run/{tenant_id}/gcp/cost/cost_billing

2. Authentication (if DISABLE_AUTH=false)
   - Verify API key
   - Extract tenant context

3. Quota Check
   - Query tenants.tenant_usage_quotas
   - Verify daily/concurrent limits

4. Template Resolution
   - Load: configs/gcp/cost/cost_billing.yml
   - Substitute variables: {tenant_id}, {date}, {admin_email}, etc.

5. Atomic Insert
   - Insert into {tenant_id}.x_meta_pipeline_runs
   - If duplicate (RUNNING|PENDING), return existing

6. Async Execution (Background)
   - Step 1: Extract GCP billing costs
     * Query source table
     * Filter by date
     * Load to tenant dataset
   - Step 2: Email notification (on failure)

7. Metadata Logging
   - Log start, end, duration
   - Log row counts, errors
   - Update final status

8. Client Monitoring
   - GET /api/v1/pipelines/runs/{pipeline_logging_id}
   - Poll for status changes

================================================================================
BIGQUERY SCHEMA
================================================================================

PIPELINE METADATA TABLE: {tenant_id}.x_meta_pipeline_runs
  - pipeline_logging_id (STRING) - Unique execution ID
  - pipeline_id (STRING) - Pipeline identifier
  - tenant_id (STRING) - Tenant identifier
  - status (STRING) - PENDING, RUNNING, COMPLETED, FAILED
  - trigger_type (STRING) - "api" or "scheduler"
  - trigger_by (STRING) - Who triggered (user_id or service name)
  - start_time (TIMESTAMP) - Execution start
  - end_time (TIMESTAMP) - Execution end (if completed)
  - duration_ms (INT64) - Duration in milliseconds
  - run_date (DATE) - Pipeline parameter date
  - parameters (JSON) - Runtime parameters

STEP EXECUTION TABLE: {tenant_id}.x_meta_step_logs
  - step_logging_id (STRING) - Unique step execution ID
  - pipeline_logging_id (STRING) - Parent pipeline ID
  - step_id (STRING) - Step identifier
  - step_name (STRING) - Human-readable step name
  - ps_type (STRING) - Processor type (gcp.bq_etl, email_notification, etc.)
  - status (STRING) - PENDING, RUNNING, COMPLETED, FAILED
  - start_time (TIMESTAMP) - Step start
  - end_time (TIMESTAMP) - Step end
  - duration_ms (INT64) - Duration
  - row_count (INT64) - Rows processed/loaded
  - error_message (STRING) - Error details if failed

COST DATA TABLE: {tenant_id}.billing_cost_daily
  - Partitioned by: ingestion_date (DAY)
  - Clustered by: billing_account_id, service_id, project_id, location_region
  - Columns: (from cost_billing.yml)
    * billing_account_id
    * service_id, service_description
    * sku_id, sku_description
    * usage_start_time, usage_end_time
    * project_id, project_name, project_number
    * location_location, location_region, location_zone
    * resource_name, resource_global_name
    * cost, currency, currency_conversion_rate
    * usage_amount, usage_unit, usage_amount_in_pricing_units
    * usage_pricing_unit
    * cost_type
    * credits_total
    * cost_at_list
    * invoice_month
    * ingestion_date
    * labels_json, system_labels_json

================================================================================
QUOTA & RATE LIMITING
================================================================================

SUBSCRIPTION PLANS:
  - STARTER: 6 pipelines/day, 3 concurrent, 2 team members, 3 providers
  - PROFESSIONAL: 25 pipelines/day, 5 concurrent, 6 team members, 6 providers
  - SCALE: 100 pipelines/day, 10 concurrent, 11 team members, 10 providers

API RATE LIMITS:
  - 100 requests per minute per tenant
  - 1000 requests per hour per tenant
  - 50 pipeline executions per minute per tenant

PIPELINE-LEVEL LIMITS:
  - Max 20-minute timeout per pipeline
  - Max 10 concurrent steps
  - Max 10 partition batches in parallel

================================================================================
ENVIRONMENT VARIABLES
================================================================================

Required:
  GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
  GCP_PROJECT_ID=gac-prod-471220
  BIGQUERY_LOCATION=US

Application:
  DISABLE_AUTH=true                    # Development mode (no auth required)
  DEFAULT_TENANT_ID=acme1281          # Default tenant for auth=false
  API_HOST=0.0.0.0
  API_PORT=8080
  ENVIRONMENT=development

Logging:
  LOG_LEVEL=INFO

Rate Limiting:
  RATE_LIMIT_ENABLED=true
  RATE_LIMIT_REQUESTS_PER_MINUTE=100
  RATE_LIMIT_PIPELINE_RUN_PER_MINUTE=50

Metadata Logging:
  METADATA_LOG_BATCH_SIZE=100
  METADATA_LOG_FLUSH_INTERVAL_SECONDS=5

================================================================================
TESTING
================================================================================

RUN FULL TEST (with onboarding):
  python test_docker_customer_billing_pipeline.py

RUN SIMPLE TEST (no onboarding):
  python test_pipeline_simple.py

MANUAL CURL TEST:
  # Health check
  curl http://localhost:8080/health

  # Trigger pipeline
  curl -X POST http://localhost:8080/api/v1/pipelines/run/acme1281/gcp/cost/cost_billing \
    -H "Content-Type: application/json" \
    -d '{"date": "2024-11-01", "trigger_by": "test"}'

  # Check status
  curl http://localhost:8080/api/v1/pipelines/runs/{pipeline_logging_id}

  # List recent runs
  curl http://localhost:8080/api/v1/pipelines/runs?limit=5

================================================================================
TROUBLESHOOTING
================================================================================

PROBLEM: 500 Internal Server Error
SOLUTION:
  1. Check application logs: tail -f application.log
  2. Verify tenant infrastructure: bq ls gac-prod-471220:{tenant_id}
  3. Check quota record: bq query 'SELECT * FROM tenants.tenant_usage_quotas'
  4. Verify template file: ls configs/gcp/cost/cost_billing.yml
  5. Check GCP credentials: gcloud auth list

PROBLEM: Pipeline Status is PENDING
SOLUTION:
  1. Check if background tasks are running: ps aux | grep uvicorn
  2. Review application logs for errors
  3. Check BigQuery job status in console
  4. Verify quotas not exceeded: bq query 'SELECT * FROM tenants.tenant_usage_quotas'
  5. Increase timeout and retry

PROBLEM: Permission Denied (BigQuery)
SOLUTION:
  1. Verify credentials file: echo $GOOGLE_APPLICATION_CREDENTIALS
  2. Test access: bq query 'SELECT 1'
  3. Check project: gcloud config get-value project
  4. Request GCP permissions for service account

PROBLEM: Template Not Found
SOLUTION:
  1. Verify file exists: ls -la configs/gcp/cost/cost_billing.yml
  2. Check file permissions: ls -la configs/gcp/cost/
  3. Verify path in config: template_path should be relative to project root
  4. Check for YAML syntax errors: python -c "import yaml; yaml.safe_load(open('configs/gcp/cost/cost_billing.yml'))"

================================================================================
MONITORING & OPERATIONS
================================================================================

DAILY PIPELINE EXECUTION:
  # List today's pipelines
  SELECT COUNT(*) as count, status
  FROM `gac-prod-471220.{tenant_id}.x_meta_pipeline_runs`
  WHERE DATE(start_time) = CURRENT_DATE()
  GROUP BY status

PERFORMANCE METRICS:
  # Average execution time per step
  SELECT
    step_id,
    AVG(TIMESTAMP_DIFF(end_time, start_time, SECOND)) as avg_duration_seconds,
    COUNT(*) as executions
  FROM `gac-prod-471220.{tenant_id}.x_meta_step_logs`
  WHERE DATE(start_time) >= CURRENT_DATE() - 7
  GROUP BY step_id
  ORDER BY avg_duration_seconds DESC

QUOTA USAGE:
  # Check today's quota usage
  SELECT
    tenant_id,
    pipelines_run_today,
    daily_limit,
    ROUND(100.0 * pipelines_run_today / daily_limit, 1) as usage_percent,
    concurrent_pipelines_running,
    concurrent_limit
  FROM `gac-prod-471220.tenants.tenant_usage_quotas`
  WHERE usage_date = CURRENT_DATE()

FAILURE ANALYSIS:
  # Find failed pipelines
  SELECT
    pipeline_logging_id,
    pipeline_id,
    start_time,
    (SELECT error_message FROM x_meta_step_logs WHERE pipeline_logging_id = p.pipeline_logging_id LIMIT 1) as error
  FROM `gac-prod-471220.{tenant_id}.x_meta_pipeline_runs` p
  WHERE status = 'FAILED'
  ORDER BY start_time DESC

================================================================================
NEXT STEPS
================================================================================

1. VERIFY SETUP
   [ ] Check application is running on port 8080
   [ ] Test health endpoint: curl http://localhost:8080/health
   [ ] Verify GCP credentials working
   [ ] Check tenant infrastructure in BigQuery

2. RUN TEST
   [ ] Execute test_pipeline_simple.py
   [ ] Monitor pipeline execution
   [ ] Query results in BigQuery
   [ ] Verify data loaded correctly

3. CUSTOMIZE FOR YOUR TENANT
   [ ] Update tenant_id in pipeline config
   [ ] Update source billing table (if different)
   [ ] Update admin email for notifications
   [ ] Adjust date parameter for historical data

4. SCHEDULE DAILY RUNS
   [ ] Use Cloud Scheduler to trigger daily
   [ ] Or use Pub/Sub for event-driven execution
   [ ] Set up alerts for failures

5. INTEGRATE WITH DASHBOARDS
   [ ] Connect Looker/Data Studio to billing_cost_daily
   [ ] Create cost analysis dashboards
   [ ] Set up automated reporting

================================================================================
CONTACT & SUPPORT
================================================================================

Documentation:
  - API Docs (live): http://localhost:8080/docs
  - README.md: Project overview
  - docs/: Architecture & design documents

Configuration:
  - Pipeline templates: configs/gcp/cost/
  - Metadata schemas: configs/metadata/schemas/
  - Dataset types: configs/system/

Troubleshooting:
  - See PIPELINE_EXECUTION_REPORT.md for detailed guide
  - See GCP_COST_BILLING_PIPELINE_QUICKSTART.md for examples

================================================================================
