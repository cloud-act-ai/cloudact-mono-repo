# ============================================
# GCP Configuration
# ============================================
# Path to Google Cloud service account JSON file
GOOGLE_APPLICATION_CREDENTIALS=/Users/gurukallam/.gcp/gac-prod-471220-e34944040b62.json

# Google Cloud Project ID
GCP_PROJECT_ID=gac-prod-471220

# BigQuery dataset location (US, EU, etc.)
BIGQUERY_LOCATION=US

# ============================================
# Application Configuration
# ============================================
# Python path for imports
PYTHONPATH=.

# Application name and version
APP_NAME=convergence-data-pipeline
APP_VERSION=1.0.0

# Environment (development, staging, production)
ENVIRONMENT=development

# Enable debug mode
DEBUG=false

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ============================================
# API Configuration
# ============================================
# API server host and port
API_HOST=0.0.0.0
API_PORT=8080

# Number of Uvicorn workers
API_WORKERS=4

# Auto-reload on code changes (development only)
API_RELOAD=false

# CORS allowed origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# CORS settings
CORS_ALLOW_CREDENTIALS=true
CORS_ALLOW_METHODS=*
CORS_ALLOW_HEADERS=*

# ============================================
# Security Configuration
# ============================================
# Disable API key authentication (for development)
DISABLE_AUTH=true

# Default tenant ID when auth is disabled
DEFAULT_TENANT_ID=acme1281

# API key hash algorithm
API_KEY_HASH_ALGORITHM=HS256

# API key secret key (change in production!)
API_KEY_SECRET_KEY=change-this-in-production-to-a-secure-random-key

# Base path for tenant secrets
SECRETS_BASE_PATH=~/.cloudact-secrets

# ============================================
# Rate Limiting
# ============================================
# Rate limit: requests per minute
RATE_LIMIT_REQUESTS_PER_MINUTE=100

# Rate limit: requests per hour
RATE_LIMIT_REQUESTS_PER_HOUR=1000

# Maximum concurrent pipeline executions
RATE_LIMIT_PIPELINE_CONCURRENCY=5

# ============================================
# Observability
# ============================================
# Enable distributed tracing
ENABLE_TRACING=true

# Enable metrics collection
ENABLE_METRICS=true

# OpenTelemetry service name
OTEL_SERVICE_NAME=convergence-api

# OpenTelemetry exporter endpoint (optional)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# ============================================
# BigQuery Configuration
# ============================================
# Maximum results per page
BQ_MAX_RESULTS_PER_PAGE=10000

# Query timeout in seconds
BQ_QUERY_TIMEOUT_SECONDS=300

# Maximum retry attempts
BQ_MAX_RETRY_ATTEMPTS=3

# ============================================
# Polars Configuration
# ============================================
# Maximum threads for Polars operations
POLARS_MAX_THREADS=8

# Streaming chunk size
POLARS_STREAMING_CHUNK_SIZE=100000

# ============================================
# Data Quality
# ============================================
# Fail pipeline on data quality errors
DQ_FAIL_ON_ERROR=false

# Store DQ results in BigQuery
DQ_STORE_RESULTS_IN_BQ=true

# ============================================
# Metadata Logging Configuration
# ============================================
# Batch size for metadata logs
METADATA_LOG_BATCH_SIZE=100

# Flush interval in seconds
METADATA_LOG_FLUSH_INTERVAL_SECONDS=5

# Maximum retries for metadata logging
METADATA_LOG_MAX_RETRIES=3

# Number of background workers for log flushing
METADATA_LOG_WORKERS=5

# Maximum queue size for buffered logs
METADATA_LOG_QUEUE_SIZE=1000

# ============================================
# Pipeline Parallel Processing
# ============================================
# Maximum parallel steps per level
PIPELINE_MAX_PARALLEL_STEPS=10

# Number of partitions to process in parallel
PIPELINE_PARTITION_BATCH_SIZE=10

# ============================================
# File Paths
# ============================================
# Base path for configuration files
CONFIGS_BASE_PATH=./configs

# Admin metadata dataset name
ADMIN_METADATA_DATASET=metadata

# Path to metadata schemas
METADATA_SCHEMAS_PATH=configs/metadata/schemas

# Path to dataset types configuration
DATASET_TYPES_CONFIG=configs/system/dataset_types.yml

# ============================================
# Distributed Lock Configuration
# ============================================
# Lock backend: 'memory' (single instance) or 'firestore' (distributed)
# Use 'firestore' for production multi-instance deployments
LOCK_BACKEND=firestore

# Lock expiration timeout in seconds (default: 1 hour)
LOCK_TIMEOUT_SECONDS=3600

# Firestore collection name for pipeline locks
FIRESTORE_LOCK_COLLECTION=pipeline_locks
