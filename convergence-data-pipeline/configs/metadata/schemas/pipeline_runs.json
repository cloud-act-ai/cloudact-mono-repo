[
  {
    "name": "pipeline_logging_id",
    "type": "STRING",
    "mode": "REQUIRED",
    "description": "Unique identifier (UUID) for this specific pipeline execution instance. Used to track and correlate all logs and metrics for a single pipeline run. Links to step_logs table."
  },
  {
    "name": "pipeline_id",
    "type": "STRING",
    "mode": "REQUIRED",
    "description": "Identifier of the pipeline definition (matches YAML filename). For example: 'google_example_pipeline', 'customer_data_pipeline'. Used for filtering and aggregating metrics by pipeline type."
  },
  {
    "name": "tenant_id",
    "type": "STRING",
    "mode": "REQUIRED",
    "description": "Tenant identifier for multi-tenancy isolation. All pipeline data is segregated by tenant. Example: 'acme1281', 'google'. Used for tenant-specific billing, access control, and data isolation."
  },
  {
    "name": "status",
    "type": "STRING",
    "mode": "REQUIRED",
    "description": "Current execution status of the pipeline. Valid values: 'PENDING' (queued for execution), 'RUNNING' (currently executing), 'COMPLETED' (successfully finished), 'FAILED' (encountered error). Used for monitoring pipeline health and SLA tracking."
  },
  {
    "name": "trigger_type",
    "type": "STRING",
    "mode": "REQUIRED",
    "description": "How the pipeline execution was initiated. Valid values: 'api' (REST API call), 'scheduler' (Cloud Scheduler/cron job), 'manual' (user-initiated), 'webhook' (external trigger), 'retry' (automatic retry after failure). Used for audit trails and understanding pipeline orchestration patterns."
  },
  {
    "name": "trigger_by",
    "type": "STRING",
    "mode": "REQUIRED",
    "description": "Identity of the user, service account, or system that triggered the pipeline. Examples: 'api_user', 'scheduler-service@project.iam', 'john@company.com'. Used for access auditing and accountability."
  },
  {
    "name": "start_time",
    "type": "TIMESTAMP",
    "mode": "REQUIRED",
    "description": "UTC timestamp when pipeline execution started (when executor.execute() was called). Used for partitioning (daily partitions for efficient querying), duration calculations, and SLA tracking. Partition key for this table."
  },
  {
    "name": "end_time",
    "type": "TIMESTAMP",
    "mode": "NULLABLE",
    "description": "UTC timestamp when pipeline execution completed (success or failure). NULL while pipeline is still running. Used to calculate execution duration (end_time - start_time) and identify stuck/zombie pipelines."
  },
  {
    "name": "duration_ms",
    "type": "INTEGER",
    "mode": "NULLABLE",
    "description": "Total execution time in milliseconds from start to completion. Pre-calculated as (end_time - start_time) for performance. NULL while pipeline is running. Used for performance analytics, SLA monitoring, and identifying slow pipelines."
  },
  {
    "name": "config_version",
    "type": "STRING",
    "mode": "NULLABLE",
    "description": "Version identifier or Git commit SHA of the pipeline configuration YAML file at execution time. Enables tracking which version of the pipeline definition was executed. Useful for debugging issues after config changes and compliance auditing."
  },
  {
    "name": "worker_instance",
    "type": "STRING",
    "mode": "NULLABLE",
    "description": "Identifier of the compute instance/pod/worker that executed this pipeline. Examples: 'worker-pod-abc123', 'instance-1', hostname. Used for debugging infrastructure issues, load balancing analysis, and identifying problematic workers."
  },
  {
    "name": "error_message",
    "type": "STRING",
    "mode": "NULLABLE",
    "description": "Human-readable error message if pipeline failed. Contains exception message, stack trace summary, or failure reason. NULL for successful runs. Used for error analysis, alerting, and debugging pipeline failures. Max recommended length: 10KB."
  },
  {
    "name": "parameters",
    "type": "JSON",
    "mode": "NULLABLE",
    "description": "Runtime parameters passed to the pipeline as flexible JSON object. Can contain any structure: dates, filters, configuration overrides, feature flags. Examples: {'date': '2025-11-14', 'batch_size': 1000, 'filters': {'region': 'US'}}. JSON type provides schema flexibility without table alterations. Used for parameterized pipeline execution and debugging with specific inputs."
  }
]
