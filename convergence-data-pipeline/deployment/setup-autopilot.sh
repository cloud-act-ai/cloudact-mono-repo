#!/bin/bash
# =============================================================================
# HANDS-OFF PIPELINE AUTOMATION SETUP
# Run this ONCE, then the system runs forever without human intervention
# =============================================================================

set -e  # Exit on error

PROJECT_ID="${GCP_PROJECT_ID:-$(gcloud config get-value project)}"
REGION="${REGION:-us-central1}"
ADMIN_API_KEY="${ADMIN_API_KEY:-$(openssl rand -hex 32)}"

echo "=========================================="
echo "ðŸš€ Setting up Hands-Off Pipeline Automation"
echo "=========================================="
echo "Project: $PROJECT_ID"
echo "Region: $REGION"
echo ""

# =============================================================================
# 1. Enable Required APIs
# =============================================================================
echo "ðŸ“¦ Enabling GCP APIs..."
gcloud services enable \
    cloudscheduler.googleapis.com \
    pubsub.googleapis.com \
    run.googleapis.com \
    cloudbuild.googleapis.com \
    bigquery.googleapis.com \
    monitoring.googleapis.com \
    logging.googleapis.com \
    --project=$PROJECT_ID

# =============================================================================
# 2. Create Pub/Sub Infrastructure
# =============================================================================
echo "ðŸ“¬ Creating Pub/Sub topics and subscriptions..."

# Main topic for pipeline tasks
gcloud pubsub topics create pipeline-tasks \
    --project=$PROJECT_ID \
    --message-retention-duration=7d \
    2>/dev/null || echo "Topic already exists"

# Dead letter topic for permanent failures
gcloud pubsub topics create pipeline-tasks-dead-letter \
    --project=$PROJECT_ID \
    --message-retention-duration=30d \
    2>/dev/null || echo "Dead letter topic already exists"

# Subscription with retry policy and dead letter queue
gcloud pubsub subscriptions create pipeline-tasks-sub \
    --topic=pipeline-tasks \
    --ack-deadline=600 \
    --min-retry-delay=10s \
    --max-retry-delay=600s \
    --dead-letter-topic=pipeline-tasks-dead-letter \
    --max-delivery-attempts=5 \
    --project=$PROJECT_ID \
    2>/dev/null || echo "Subscription already exists"

# Dead letter subscription (for monitoring failures)
gcloud pubsub subscriptions create pipeline-tasks-dead-letter-sub \
    --topic=pipeline-tasks-dead-letter \
    --project=$PROJECT_ID \
    2>/dev/null || echo "Dead letter subscription already exists"

# =============================================================================
# 3. Deploy API (Publisher)
# =============================================================================
echo "ðŸŒ Deploying API service..."
gcloud run deploy convergence-api \
    --source=. \
    --region=$REGION \
    --platform=managed \
    --allow-unauthenticated \
    --memory=2Gi \
    --cpu=2 \
    --min-instances=1 \
    --max-instances=10 \
    --timeout=300 \
    --set-env-vars="GCP_PROJECT_ID=$PROJECT_ID,ADMIN_API_KEY=$ADMIN_API_KEY" \
    --project=$PROJECT_ID

API_URL=$(gcloud run services describe convergence-api \
    --region=$REGION \
    --platform=managed \
    --format='value(status.url)' \
    --project=$PROJECT_ID)

echo "API deployed: $API_URL"

# =============================================================================
# 4. Deploy Worker (Subscriber)
# =============================================================================
echo "âš™ï¸  Deploying worker service..."

# Create worker Dockerfile if not exists
cat > Dockerfile.worker <<'EOF'
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
ENV PYTHONUNBUFFERED=1
CMD ["python", "-c", "from src.core.pubsub.worker import PipelineWorker; PipelineWorker(max_concurrent=100).start()"]
EOF

gcloud run deploy convergence-worker \
    --source=. \
    --dockerfile=Dockerfile.worker \
    --region=$REGION \
    --platform=managed \
    --no-allow-unauthenticated \
    --memory=4Gi \
    --cpu=4 \
    --min-instances=1 \
    --max-instances=50 \
    --timeout=3600 \
    --concurrency=2 \
    --set-env-vars="GCP_PROJECT_ID=$PROJECT_ID" \
    --project=$PROJECT_ID

# =============================================================================
# 5. Create Cloud Scheduler Job (Monthly Trigger)
# =============================================================================
echo "â° Creating Cloud Scheduler job (runs MONTHLY on 1st at midnight UTC)..."

# Get all tenant IDs from BigQuery
TENANT_IDS=$(bq query --use_legacy_sql=false --format=csv --max_rows=100000 \
    "SELECT DISTINCT schema_name FROM \`$PROJECT_ID.INFORMATION_SCHEMA.SCHEMATA\`
     WHERE schema_name NOT IN ('information_schema', 'metadata', 'pg_catalog')" \
    | tail -n +2 | jq -R -s -c 'split("\n")[:-1]')

# Create scheduler payload
cat > /tmp/scheduler-payload.json <<EOF
{
  "tenant_ids": $TENANT_IDS,
  "pipeline_id": "p_openai_billing",
  "parameters": {
    "date": "{{ .FormatTime .ScheduleTime \"2006-01-02\" }}",
    "trigger_by": "cloud_scheduler"
  },
  "randomize_delay": true,
  "max_jitter_seconds": 3600
}
EOF

# Create or update scheduler job (runs MONTHLY on 1st at midnight UTC)
gcloud scheduler jobs create http pipeline-monthly-batch \
    --location=$REGION \
    --schedule="0 0 1 * *" \
    --time-zone="UTC" \
    --uri="$API_URL/pipelines/batch/publish" \
    --http-method=POST \
    --headers="X-Admin-Key=$ADMIN_API_KEY,Content-Type=application/json" \
    --message-body-from-file=/tmp/scheduler-payload.json \
    --attempt-deadline=1800s \
    --max-retry-attempts=3 \
    --min-backoff-duration=10s \
    --max-backoff-duration=3600s \
    --project=$PROJECT_ID \
    2>/dev/null || \
gcloud scheduler jobs update http pipeline-monthly-batch \
    --location=$REGION \
    --schedule="0 0 1 * *" \
    --time-zone="UTC" \
    --uri="$API_URL/pipelines/batch/publish" \
    --http-method=POST \
    --headers="X-Admin-Key=$ADMIN_API_KEY,Content-Type=application/json" \
    --message-body-from-file=/tmp/scheduler-payload.json \
    --attempt-deadline=1800s \
    --max-retry-attempts=3 \
    --min-backoff-duration=10s \
    --max-backoff-duration=3600s \
    --project=$PROJECT_ID

# =============================================================================
# 6. Set up Monitoring & Alerts
# =============================================================================
echo "ðŸ“Š Creating monitoring dashboard and alerts..."

# Alert policy for dead letter queue (only alert on permanent failures)
cat > /tmp/alert-policy.json <<EOF
{
  "displayName": "Pipeline Permanent Failures",
  "conditions": [
    {
      "displayName": "Dead Letter Queue has messages",
      "conditionThreshold": {
        "filter": "resource.type=\"pubsub_subscription\" AND resource.labels.subscription_id=\"pipeline-tasks-dead-letter-sub\" AND metric.type=\"pubsub.googleapis.com/subscription/num_undelivered_messages\"",
        "comparison": "COMPARISON_GT",
        "thresholdValue": 10,
        "duration": "300s",
        "aggregations": [
          {
            "alignmentPeriod": "60s",
            "perSeriesAligner": "ALIGN_MEAN"
          }
        ]
      }
    }
  ],
  "combiner": "OR",
  "enabled": true,
  "notificationChannels": [],
  "alertStrategy": {
    "autoClose": "604800s"
  }
}
EOF

gcloud alpha monitoring policies create --policy-from-file=/tmp/alert-policy.json \
    --project=$PROJECT_ID \
    2>/dev/null || echo "Alert policy already exists"

# Create monitoring dashboard
cat > /tmp/dashboard.json <<EOF
{
  "displayName": "Pipeline Autopilot Dashboard",
  "mosaicLayout": {
    "columns": 12,
    "tiles": [
      {
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Pub/Sub Queue Depth",
          "xyChart": {
            "dataSets": [{
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "resource.type=\"pubsub_subscription\" AND resource.labels.subscription_id=\"pipeline-tasks-sub\" AND metric.type=\"pubsub.googleapis.com/subscription/num_undelivered_messages\""
                }
              }
            }]
          }
        }
      },
      {
        "xPos": 6,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Worker Instance Count",
          "xyChart": {
            "dataSets": [{
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "resource.type=\"cloud_run_revision\" AND resource.labels.service_name=\"convergence-worker\" AND metric.type=\"run.googleapis.com/container/instance_count\""
                }
              }
            }]
          }
        }
      },
      {
        "yPos": 4,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "Dead Letter Queue (Failures)",
          "xyChart": {
            "dataSets": [{
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "resource.type=\"pubsub_subscription\" AND resource.labels.subscription_id=\"pipeline-tasks-dead-letter-sub\" AND metric.type=\"pubsub.googleapis.com/subscription/num_undelivered_messages\""
                }
              }
            }]
          }
        }
      },
      {
        "xPos": 6,
        "yPos": 4,
        "width": 6,
        "height": 4,
        "widget": {
          "title": "BigQuery Queries/sec",
          "xyChart": {
            "dataSets": [{
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "resource.type=\"global\" AND metric.type=\"bigquery.googleapis.com/query/count\""
                }
              }
            }]
          }
        }
      }
    ]
  }
}
EOF

gcloud monitoring dashboards create --config-from-file=/tmp/dashboard.json \
    --project=$PROJECT_ID \
    2>/dev/null || echo "Dashboard already exists"

# =============================================================================
# 7. Create Auto-Healing Script (runs if worker crashes)
# =============================================================================
echo "ðŸ”§ Setting up auto-healing..."

# Cloud Scheduler job to restart worker if crashed (runs every 5 minutes)
gcloud scheduler jobs create http worker-health-check \
    --location=$REGION \
    --schedule="*/5 * * * *" \
    --time-zone="UTC" \
    --uri="https://$REGION-run.googleapis.com/apis/serving.knative.dev/v1/namespaces/$PROJECT_ID/services/convergence-worker" \
    --http-method=GET \
    --oauth-service-account-email="$PROJECT_ID@appspot.gserviceaccount.com" \
    --project=$PROJECT_ID \
    2>/dev/null || echo "Health check already exists"

# =============================================================================
# DONE!
# =============================================================================
echo ""
echo "=========================================="
echo "âœ… HANDS-OFF AUTOMATION COMPLETE!"
echo "=========================================="
echo ""
echo "ðŸŽ¯ What happens next (automatically):"
echo "  1. MONTHLY (1st at midnight UTC): Cloud Scheduler triggers"
echo "  2. 10k tasks published to Pub/Sub with random delays"
echo "  3. Workers auto-scale 1â†’50 based on queue depth"
echo "  4. Pipelines execute (100-300 concurrent)"
echo "  5. Failures auto-retry (max 5 attempts)"
echo "  6. Permanent failures â†’ Dead Letter Queue"
echo "  7. Workers scale down to 1 when done"
echo "  8. REPEAT NEXT MONTH (forever)"
echo ""
echo "ðŸ“Š Monitoring Dashboard:"
echo "  https://console.cloud.google.com/monitoring/dashboards?project=$PROJECT_ID"
echo ""
echo "â° Cloud Scheduler (manual test or view schedule):"
echo "  https://console.cloud.google.com/cloudscheduler?project=$PROJECT_ID"
echo ""
echo "ðŸ”” Alerts:"
echo "  You'll be notified if >10 permanent failures occur"
echo ""
echo "ðŸ”‘ Admin API Key (save this):"
echo "  $ADMIN_API_KEY"
echo ""
echo "ðŸ“‹ NEXT STEPS:"
echo "  1. See MONTHLY_TESTING_GUIDE.md for manual testing"
echo "  2. See GCP_NAVIGATION_GUIDE.md for console navigation"
echo ""
echo "ðŸš€ System is now FULLY AUTOMATED - you do NOTHING!"
echo "  Next automatic run: 1st of next month at midnight UTC"
echo "=========================================="

# Clean up temp files
rm -f /tmp/scheduler-payload.json /tmp/alert-policy.json /tmp/dashboard.json Dockerfile.worker
