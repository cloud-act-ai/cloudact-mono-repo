# ============================================
# Alert Manager Configuration
# ============================================
# PagerDuty, Slack, and Email alert routing

global:
  # Default alert resolution timeout
  resolve_timeout: 5m

  # PagerDuty configuration
  pagerduty_url: https://events.pagerduty.com/v2/enqueue

  # Slack configuration
  slack_api_url: https://slack.com/api/chat.postMessage

  # SMTP configuration
  smtp_smarthost: ${EMAIL_SMTP_HOST}:${EMAIL_SMTP_PORT}
  smtp_from: alerts@cloudact.io
  smtp_auth_username: ${EMAIL_SMTP_USERNAME}
  smtp_auth_password: ${EMAIL_SMTP_PASSWORD}
  smtp_require_tls: true

# ============================================
# Alert Templates
# ============================================
templates:
  - /etc/alertmanager/templates/*.tmpl

# ============================================
# Alert Routing
# ============================================
route:
  # Default receiver for all alerts
  receiver: default-notifications

  # Group alerts by these labels
  group_by:
    - alertname
    - severity
    - component

  # Wait time before sending initial notification
  group_wait: 30s

  # Wait time before sending updated notification
  group_interval: 5m

  # Wait time before re-sending notification
  repeat_interval: 4h

  # Routes for specific alert types
  routes:
    # Critical alerts - PagerDuty + Slack + Email
    - match:
        severity: critical
      receiver: pagerduty-critical
      continue: true
      group_wait: 10s
      repeat_interval: 30m

    - match:
        severity: critical
      receiver: slack-critical
      continue: true

    - match:
        severity: critical
      receiver: email-critical

    # High severity alerts - Slack + Email
    - match:
        severity: high
      receiver: slack-high
      continue: true
      group_wait: 30s
      repeat_interval: 1h

    - match:
        severity: high
      receiver: email-high

    # Warning alerts - Slack only
    - match:
        severity: warning
      receiver: slack-warnings
      group_wait: 1m
      repeat_interval: 4h

    # Security alerts - immediate escalation
    - match:
        component: security
      receiver: pagerduty-security
      continue: true
      group_wait: 5s
      repeat_interval: 15m

    - match:
        component: security
      receiver: slack-security
      continue: true

    # Pipeline alerts
    - match:
        component: pipeline
      receiver: slack-pipeline
      group_wait: 2m
      repeat_interval: 2h

    # BigQuery alerts
    - match:
        component: bigquery
      receiver: slack-infrastructure

    # System resource alerts
    - match:
        component: system
      receiver: slack-infrastructure

# ============================================
# Alert Receivers
# ============================================
receivers:
  # Default receiver
  - name: default-notifications
    slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: "#alerts-general"
        title: "{{ .GroupLabels.alertname }}"
        text: "{{ range .Alerts }}{{ .Annotations.description }}\n{{ end }}"

  # PagerDuty - Critical alerts
  - name: pagerduty-critical
    pagerduty_configs:
      - service_key: ${PAGERDUTY_SERVICE_KEY_CRITICAL}
        severity: critical
        description: "{{ .GroupLabels.alertname }}"
        details:
          firing: "{{ .Alerts.Firing | len }}"
          resolved: "{{ .Alerts.Resolved | len }}"
          group_labels: "{{ .GroupLabels }}"
        client: Convergence Data Pipeline
        client_url: https://grafana.cloudact.io

  # PagerDuty - Security alerts
  - name: pagerduty-security
    pagerduty_configs:
      - service_key: ${PAGERDUTY_SERVICE_KEY_SECURITY}
        severity: critical
        description: "SECURITY: {{ .GroupLabels.alertname }}"
        details:
          firing: "{{ .Alerts.Firing | len }}"
          resolved: "{{ .Alerts.Resolved | len }}"
          group_labels: "{{ .GroupLabels }}"
        client: Convergence Data Pipeline
        client_url: https://grafana.cloudact.io

  # Slack - Critical alerts
  - name: slack-critical
    slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: "#alerts-critical"
        username: "Alert Manager"
        icon_emoji: ":rotating_light:"
        title: "@channel CRITICAL: {{ .GroupLabels.alertname }}"
        text: |
          *Status:* {{ .Status | toUpper }}
          *Severity:* {{ .GroupLabels.severity | toUpper }}
          *Component:* {{ .GroupLabels.component }}

          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
        color: danger
        send_resolved: true

  # Slack - High severity alerts
  - name: slack-high
    slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: "#alerts-high"
        username: "Alert Manager"
        icon_emoji: ":warning:"
        title: "@here HIGH: {{ .GroupLabels.alertname }}"
        text: |
          *Status:* {{ .Status | toUpper }}
          *Severity:* {{ .GroupLabels.severity | toUpper }}
          *Component:* {{ .GroupLabels.component }}

          {{ range .Alerts }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
        color: warning
        send_resolved: true

  # Slack - Warnings
  - name: slack-warnings
    slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: "#alerts-warnings"
        username: "Alert Manager"
        icon_emoji: ":large_orange_diamond:"
        title: "WARNING: {{ .GroupLabels.alertname }}"
        text: |
          *Status:* {{ .Status | toUpper }}
          *Component:* {{ .GroupLabels.component }}

          {{ range .Alerts }}
          {{ .Annotations.description }}
          {{ end }}
        color: "#FFA500"
        send_resolved: true

  # Slack - Security alerts
  - name: slack-security
    slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: "#security-alerts"
        username: "Security Alert"
        icon_emoji: ":lock:"
        title: "@channel SECURITY ALERT: {{ .GroupLabels.alertname }}"
        text: |
          *Status:* {{ .Status | toUpper }}
          *Severity:* {{ .GroupLabels.severity | toUpper }}

          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
        color: danger
        send_resolved: true

  # Slack - Pipeline alerts
  - name: slack-pipeline
    slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: "#alerts-pipeline"
        username: "Pipeline Alert"
        icon_emoji: ":pipeline:"
        title: "Pipeline Alert: {{ .GroupLabels.alertname }}"
        text: |
          *Status:* {{ .Status | toUpper }}
          *Severity:* {{ .GroupLabels.severity | toUpper }}

          {{ range .Alerts }}
          {{ .Annotations.description }}
          {{ end }}
        send_resolved: true

  # Slack - Infrastructure alerts
  - name: slack-infrastructure
    slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: "#alerts-infrastructure"
        username: "Infrastructure Alert"
        icon_emoji: ":cloud:"
        title: "Infrastructure Alert: {{ .GroupLabels.alertname }}"
        text: |
          *Status:* {{ .Status | toUpper }}
          *Component:* {{ .GroupLabels.component }}

          {{ range .Alerts }}
          {{ .Annotations.description }}
          {{ end }}
        send_resolved: true

  # Email - Critical alerts
  - name: email-critical
    email_configs:
      - to: oncall@cloudact.io,ops-team@cloudact.io
        from: alerts@cloudact.io
        smarthost: ${EMAIL_SMTP_HOST}:${EMAIL_SMTP_PORT}
        auth_username: ${EMAIL_SMTP_USERNAME}
        auth_password: ${EMAIL_SMTP_PASSWORD}
        headers:
          Subject: "[CRITICAL] {{ .GroupLabels.alertname }}"
        html: |
          <h2>CRITICAL ALERT</h2>
          <p><strong>Status:</strong> {{ .Status | toUpper }}</p>
          <p><strong>Severity:</strong> {{ .GroupLabels.severity | toUpper }}</p>
          <p><strong>Component:</strong> {{ .GroupLabels.component }}</p>

          {{ range .Alerts }}
          <hr>
          <h3>{{ .Labels.alertname }}</h3>
          <p><strong>Description:</strong> {{ .Annotations.description }}</p>
          <p><strong>Runbook:</strong> <a href="{{ .Annotations.runbook_url }}">{{ .Annotations.runbook_url }}</a></p>
          {{ end }}
        send_resolved: true

  # Email - High severity alerts
  - name: email-high
    email_configs:
      - to: ops-team@cloudact.io
        from: alerts@cloudact.io
        smarthost: ${EMAIL_SMTP_HOST}:${EMAIL_SMTP_PORT}
        auth_username: ${EMAIL_SMTP_USERNAME}
        auth_password: ${EMAIL_SMTP_PASSWORD}
        headers:
          Subject: "[HIGH] {{ .GroupLabels.alertname }}"
        html: |
          <h2>HIGH SEVERITY ALERT</h2>
          <p><strong>Status:</strong> {{ .Status | toUpper }}</p>
          <p><strong>Component:</strong> {{ .GroupLabels.component }}</p>

          {{ range .Alerts }}
          <hr>
          <p><strong>Description:</strong> {{ .Annotations.description }}</p>
          <p><strong>Runbook:</strong> <a href="{{ .Annotations.runbook_url }}">{{ .Annotations.runbook_url }}</a></p>
          {{ end }}
        send_resolved: true

# ============================================
# Inhibition Rules
# ============================================
# Suppress certain alerts when others are firing
inhibit_rules:
  # If ServiceDown is firing, inhibit all other alerts for that service
  - source_match:
      alertname: ServiceDown
    target_match_re:
      alertname: .*
    equal:
      - instance

  # If HighErrorRate is firing, inhibit latency alerts
  - source_match:
      alertname: HighErrorRate
    target_match:
      alertname: HighLatencyP95
    equal:
      - instance

  # If CriticalCPUUsage is firing, inhibit HighCPUUsage
  - source_match:
      alertname: CriticalCPUUsage
    target_match:
      alertname: HighCPUUsage
    equal:
      - instance

  # If CriticalMemoryUsage is firing, inhibit HighMemoryUsage
  - source_match:
      alertname: CriticalMemoryUsage
    target_match:
      alertname: HighMemoryUsage
    equal:
      - instance

# ============================================
# Time-based Muting
# ============================================
# Example: Mute non-critical alerts during maintenance windows
# These would typically be configured via the AlertManager UI

# ============================================
# Alert Grouping Configuration
# ============================================
# Alerts are grouped by alertname, severity, and component
# This prevents alert fatigue from similar alerts

# ============================================
# Notification Rate Limiting
# ============================================
# Alerts are rate-limited by repeat_interval to prevent spam
# Critical: 30 minutes
# High: 1 hour
# Warning: 4 hours
