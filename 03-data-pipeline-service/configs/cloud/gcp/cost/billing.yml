# GCP Billing Pipeline
# Extracts billing data from GCP billing export, stores in BigQuery
#
# Schedule: Daily at 04:00 UTC
# Tables: cloud_gcp_billing_raw_daily
#
# Usage: POST /api/v1/pipelines/run/{org_slug}/gcp/cost/billing
#
# PREREQUISITES:
# 1. Organization must have a valid GCP_SA integration configured
# 2. The GCP Service Account must have BigQuery read access to billing export
# 3. billing_export_table must be configured in the integration metadata
#
# GCP BILLING EXPORT TABLE TYPES (configured in Settings → Integrations → GCP):
# - billing_export_table: Standard export (gcp_billing_export_v1_*) - REQUIRED for cost data
# - detailed_export_table: Resource export (gcp_billing_export_resource_v1_*)
# - pricing_export_table: Pricing catalog (cloud_pricing_export)
# - committed_use_discount_table: CUD data for commitment analysis

pipeline_id: "{org_slug}-gcp-billing"
name: "GCP Billing Sync"
description: "Extract GCP billing costs and store in BigQuery"
provider: "gcp"
domain: "cost"
version: "15.0.0"

# Schedule configuration (cron expression: daily at 04:00 UTC)
schedule: "0 4 * * *"

variables:
  org_slug: "{org_slug}"
  provider: "gcp"
  # NOTE: source_billing_table should be configured per-org via integration settings
  # This comes from the org's GCP integration credentials
  source_billing_table: "{billing_export_table}"
  # Multi-tenant: Use customer's GCP Service Account credentials to access their billing export
  use_org_credentials: "true"

steps:
  # Step 1 (Optional): Decrypt GCP credentials for customer's billing data
  - step_id: "decrypt_credentials"
    name: "Decrypt Credentials"
    ps_type: "integrations.kms_decrypt"
    description: "Load organization GCP credentials"
    enabled: "{use_org_credentials}"
    config:
      provider: "GCP_SA"
      require_valid: true
      context_key: "gcp_sa_json"
    on_failure: "continue"

  # Step 2: Extract billing data from GCP billing export
  - step_id: "extract_billing"
    name: "Extract Billing Data"
    ps_type: "cloud.gcp.external_bq_extractor"
    description: "Extract GCP billing costs for date range"
    depends_on:
      - "decrypt_credentials"
    timeout_minutes: 20

    source:
      # Multi-tenant: Use customer's GCP SA credentials to read their billing export
      use_org_credentials: true
      query: |
        SELECT
          billing_account_id,
          service.id AS service_id,
          service.description AS service_description,
          sku.id AS sku_id,
          sku.description AS sku_description,
          usage_start_time,
          usage_end_time,
          project.id AS project_id,
          project.name AS project_name,
          project.number AS project_number,
          location.location AS location_location,
          location.region AS location_region,
          location.zone AS location_zone,
          -- NOTE: resource.name and resource.global_name only exist in detailed export (gcp_billing_export_resource_v1_*)
          -- They are NOT available in standard export (gcp_billing_export_v1_*)
          -- Use NULL placeholders for compatibility with destination schema
          CAST(NULL AS STRING) AS resource_name,
          CAST(NULL AS STRING) AS resource_global_name,
          cost,
          currency,
          currency_conversion_rate,
          usage.amount AS usage_amount,
          usage.unit AS usage_unit,
          usage.amount_in_pricing_units AS usage_amount_in_pricing_units,
          usage.pricing_unit AS usage_pricing_unit,
          cost_type,
          (SELECT IFNULL(SUM(amount), 0.0) FROM UNNEST(credits)) AS credits_total,
          cost_at_list,
          invoice.month AS invoice_month,
          CURRENT_DATE() AS ingestion_date,
          TO_JSON_STRING(ARRAY(SELECT AS STRUCT key, value FROM UNNEST(labels))) AS labels_json,
          TO_JSON_STRING(ARRAY(SELECT AS STRUCT key, value FROM UNNEST(system_labels))) AS system_labels_json
        FROM `{source_billing_table}`
        WHERE DATE(usage_start_time) = '{date}'

    # Destination: Always CloudAct project + org dataset ({org_slug}_{env})
    # Processor enforces standard destination via settings.get_org_dataset_name()
    destination:
      dataset_type: "gcp"
      table: "cloud_gcp_billing_raw_daily"
      write_mode: "append"
      schema_template: "billing_cost"
      table_config:
        time_partitioning:
          field: "ingestion_date"
          type: "DAY"
          expiration_days: 730
        clustering_fields:
          - "billing_account_id"
          - "service_id"
          - "project_id"
          - "location_region"

  # Step 3: Notification on failure
  - step_id: "notify_on_failure"
    name: "Notify on Failure"
    ps_type: "notify_systems.email_notification"
    description: "Send failure notification"
    trigger: "on_failure"
    config:
      to_emails:
        - "{admin_email}"
        - "data-ops@example.com"
      subject: "[ALERT] GCP Billing Pipeline Failed - {org_slug}"
      message: |
        ALERT: The GCP billing pipeline has failed!

        Organization: {org_slug}
        Pipeline: {pipeline_id}
        Run Date: {date}

requires_auth: true
auth_type: "org_api_key"

tags:
  - billing
  - gcp
  - cost
  - daily
category: "cloud_cost"
