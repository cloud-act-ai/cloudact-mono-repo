# GCP Billing Pipeline
# Extracts billing data from GCP billing export, stores in BigQuery
#
# Schedule: Daily at 04:00 UTC (processes yesterday's data by default)
# Tables: cloud_gcp_billing_raw_daily
#
# Usage:
#   POST /api/v1/pipelines/run/{org_slug}/gcp/cost/billing
#   POST /api/v1/pipelines/run/{org_slug}/gcp/cost/billing?start_date=2026-01-01&end_date=2026-01-22
#
# BEHAVIOR:
# - Default: Processes yesterday's data (T-1) based on export_time
# - Idempotent: Deletes existing data for export_time range before inserting
# - Handles late-arriving corrections: Uses export_time (not usage_start_time)
#   so corrections to older usage dates are captured when they're exported
# - Supports date ranges for backfill
#
# PREREQUISITES:
# 1. Organization must have a valid GCP_SA integration configured
# 2. The GCP Service Account must have BigQuery read access to billing export
# 3. billing_export_table must be configured in the integration metadata
#
# GCP BILLING EXPORT TABLE TYPES (configured in Settings → Integrations → GCP):
# - billing_export_table: Standard export (gcp_billing_export_v1_*) - REQUIRED for cost data
# - detailed_export_table: Resource export (gcp_billing_export_resource_v1_*)
# - pricing_export_table: Pricing catalog (cloud_pricing_export)
# - committed_use_discount_table: CUD data for commitment analysis

pipeline_id: "{org_slug}-gcp-billing"
name: "GCP Billing Sync"
description: "Extract GCP billing costs and store in BigQuery (idempotent, replaces existing data)"
provider: "gcp"
domain: "cost"
version: "18.0.0"

# Schedule configuration (cron expression: daily at 04:00 UTC)
schedule: "0 4 * * *"

variables:
  org_slug: "{org_slug}"
  provider: "gcp"
  # NOTE: source_billing_table should be configured per-org via integration settings
  # This comes from the org's GCP integration credentials
  source_billing_table: "{billing_export_table}"
  # Multi-tenant: Use customer's GCP Service Account credentials to access their billing export
  use_org_credentials: "true"
  # Environment for hierarchy lookup - auto-filled by processor
  # Maps: development->local, staging->stage, production->prod
  # Default date: Yesterday (T-1) - overridden by request parameters
  # Executor auto-calculates if not provided
  default_date_offset: "-1"

steps:
  # Step 1 (Optional): Decrypt GCP credentials for customer's billing data
  - step_id: "decrypt_credentials"
    name: "Decrypt Credentials"
    ps_type: "integrations.kms_decrypt"
    description: "Load organization GCP credentials"
    enabled: "{use_org_credentials}"
    config:
      provider: "GCP_SA"
      require_valid: true
      context_key: "gcp_sa_json"
    on_failure: "continue"

  # Step 2: Delete existing data for export_time range (idempotent - allows re-runs)
  # NOTE: Uses export_time (not usage_start_time) to handle late-arriving corrections
  # GCP may export corrections for older usage dates with new export_time
  - step_id: "delete_existing_data"
    name: "Delete Existing Data"
    ps_type: "generic.bq_execute"
    description: "Delete existing billing data for export_time range to ensure idempotency"
    depends_on:
      - "decrypt_credentials"
    config:
      query: |
        DELETE FROM `{gcp_project_id}.{org_slug}_{environment}.cloud_gcp_billing_raw_daily`
        WHERE DATE(export_time) BETWEEN @start_date AND @end_date
          AND x_org_slug = @org_slug
      parameters:
        - name: "start_date"
          type: "DATE"
          value: "{start_date}"
        - name: "end_date"
          type: "DATE"
          value: "{end_date}"
        - name: "org_slug"
          type: "STRING"
          value: "{org_slug}"
    on_failure: "continue"  # Continue even if table doesn't exist yet

  # Step 3: Extract billing data from GCP billing export
  - step_id: "extract_billing"
    name: "Extract Billing Data"
    ps_type: "cloud.gcp.external_bq_extractor"
    description: "Extract GCP billing costs for date range"
    depends_on:
      - "delete_existing_data"
    timeout_minutes: 20

    source:
      # Multi-tenant: Use customer's GCP SA credentials to read their billing export
      use_org_credentials: true
      query: |
        SELECT
          -- Primary identifiers
          billing_account_id,
          service.id AS service_id,
          service.description AS service_description,
          sku.id AS sku_id,
          sku.description AS sku_description,

          -- Time fields
          usage_start_time,
          usage_end_time,
          export_time,

          -- Project information
          project.id AS project_id,
          project.name AS project_name,
          CAST(project.number AS STRING) AS project_number,
          project.ancestry_numbers AS project_ancestry_numbers,
          TO_JSON_STRING(ARRAY(SELECT AS STRUCT resource_name, display_name FROM UNNEST(project.ancestors))) AS project_ancestors_json,
          TO_JSON_STRING(ARRAY(SELECT AS STRUCT key, value FROM UNNEST(project.labels))) AS project_labels_json,

          -- Location (ALL fields)
          location.location AS location_location,
          location.country AS location_country,
          location.region AS location_region,
          location.zone AS location_zone,

          -- Resource (only in detailed export - NULL for standard)
          CAST(NULL AS STRING) AS resource_name,
          CAST(NULL AS STRING) AS resource_global_name,

          -- Cost fields
          cost,
          cost_type,
          cost_at_list,
          cost_at_effective_price_default,
          cost_at_list_consumption_model,
          currency,
          currency_conversion_rate,

          -- Usage metrics
          usage.amount AS usage_amount,
          usage.unit AS usage_unit,
          usage.amount_in_pricing_units AS usage_amount_in_pricing_units,
          usage.pricing_unit AS usage_pricing_unit,

          -- Price information
          CAST(price.effective_price AS FLOAT64) AS price_effective_price,
          CAST(price.list_price AS FLOAT64) AS price_list_price,
          price.unit AS price_unit,
          CAST(price.tier_start_amount AS FLOAT64) AS price_tier_start_amount,
          CAST(price.pricing_unit_quantity AS FLOAT64) AS price_pricing_unit_quantity,

          -- Credits (FULL DETAIL - not just total)
          (SELECT IFNULL(SUM(amount), 0.0) FROM UNNEST(credits)) AS credits_total,
          TO_JSON_STRING(ARRAY(SELECT AS STRUCT name, amount, full_name, id, type FROM UNNEST(credits))) AS credits_json,

          -- Invoice
          invoice.month AS invoice_month,
          invoice.publisher_type AS invoice_publisher_type,

          -- Transaction & seller
          transaction_type,
          seller_name,

          -- Adjustment info (for corrections/refunds)
          TO_JSON_STRING(STRUCT(
            adjustment_info.id AS id,
            adjustment_info.description AS description,
            adjustment_info.mode AS mode,
            adjustment_info.type AS type
          )) AS adjustment_info_json,

          -- Consumption model (for committed use)
          TO_JSON_STRING(STRUCT(
            consumption_model.id AS id,
            consumption_model.description AS description
          )) AS consumption_model_json,

          -- Labels and tags
          TO_JSON_STRING(ARRAY(SELECT AS STRUCT key, value FROM UNNEST(labels))) AS labels_json,
          TO_JSON_STRING(ARRAY(SELECT AS STRUCT key, value FROM UNNEST(system_labels))) AS system_labels_json,
          TO_JSON_STRING(ARRAY(SELECT AS STRUCT key, value, inherited, namespace FROM UNNEST(tags))) AS tags_json,

          -- Pipeline lineage fields (x_* standard)
          -- Note: Using variable substitution {var} as external_bq_extractor uses string replacement
          -- Available vars: org_slug, pipeline_id, run_id, start_date, end_date, date
          GENERATE_UUID() AS x_ingestion_id,
          CURRENT_DATE() AS x_ingestion_date,
          '{org_slug}' AS x_org_slug,
          '{pipeline_id}' AS x_pipeline_id,
          CAST(NULL AS STRING) AS x_credential_id,
          DATE('{start_date}') AS x_pipeline_run_date,
          '{run_id}' AS x_run_id,
          CURRENT_TIMESTAMP() AS x_ingested_at,
          'GCP' AS x_cloud_provider,
          billing_account_id AS x_cloud_account_id,

          -- Hierarchy mapping (GCP project = hierarchy entity)
          -- NOTE: Hierarchy enrichment is done in post-processing with CloudAct credentials
          -- because this query runs under customer's SA which can't access CloudAct's tables
          project.id AS x_hierarchy_entity_id,
          project.name AS x_hierarchy_entity_name,
          CAST(NULL AS STRING) AS x_hierarchy_level_code,
          CAST(NULL AS STRING) AS x_hierarchy_path,
          CAST(NULL AS STRING) AS x_hierarchy_path_names,
          CAST(NULL AS TIMESTAMP) AS x_hierarchy_validated_at

        FROM `{source_billing_table}` b
        -- Filter by export_time (not usage_start_time) to capture late-arriving corrections
        -- GCP may export adjustments for older usage dates with new export_time
        -- Note: Using variable substitution {start_date}/{end_date} as processor handles escaping
        WHERE DATE(b.export_time) BETWEEN DATE('{start_date}') AND DATE('{end_date}')

    # Destination: Always CloudAct project + org dataset ({org_slug}_{env})
    # Processor enforces standard destination via settings.get_org_dataset_name()
    destination:
      dataset_type: "gcp"
      table: "cloud_gcp_billing_raw_daily"
      write_mode: "append"
      schema_template: "billing_cost"
      table_config:
        time_partitioning:
          field: "x_ingestion_date"
          type: "DAY"
          expiration_days: 730
        clustering_fields:
          - "billing_account_id"
          - "service_id"
          - "project_id"
          - "location_region"

  # Step 4: Convert to FOCUS 1.3 format
  # Runs the stored procedure to transform raw billing data to FOCUS standard
  - step_id: "convert_to_focus"
    name: "Convert to FOCUS 1.3"
    ps_type: "generic.procedure_executor"
    description: "Transform raw billing data to FOCUS 1.3 standard format"
    depends_on:
      - "extract_billing"
    config:
      procedure:
        name: sp_cloud_1_convert_to_focus
        dataset: organizations
      parameters:
        - name: p_project_id
          type: STRING
          value: "${project_id}"
        - name: p_dataset_id
          type: STRING
          value: "${org_dataset}"
        - name: p_start_date
          type: DATE
          value: "${start_date}"
        - name: p_end_date
          type: DATE
          value: "${end_date}"
        - name: p_provider
          type: STRING
          value: "gcp"
        - name: p_pipeline_id
          type: STRING
          value: "${pipeline_id}"
        - name: p_credential_id
          type: STRING
          value: "${credential_id}"
        - name: p_run_id
          type: STRING
          value: "${run_id}"
    timeout_minutes: 30
    on_failure: "stop"

  # Step 5: Notification on failure
  - step_id: "notify_on_failure"
    name: "Notify on Failure"
    ps_type: "notify_systems.email_notification"
    description: "Send failure notification"
    trigger: "on_failure"
    config:
      to_emails:
        - "{admin_email}"
        - "data-ops@example.com"
      subject: "[ALERT] GCP Billing Pipeline Failed - {org_slug}"
      message: |
        ALERT: The GCP billing pipeline has failed!

        Organization: {org_slug}
        Pipeline: {pipeline_id}
        Run Date: {date}

requires_auth: true
auth_type: "org_api_key"

tags:
  - billing
  - gcp
  - cost
  - daily
category: "cloud_cost"
