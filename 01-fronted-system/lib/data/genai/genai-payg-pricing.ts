/**
 * GenAI PAYG (Pay-As-You-Go) Pricing Data
 * Token-based pricing for LLM providers
 * Source: ZZ-PRE-ANALLISYS/data/pricing/genai_payg_pricing.csv
 */

export interface GenAIPAYGPricing {
  provider: string;
  model: string;
  model_family: string;
  model_version: string;
  region: string;
  input_per_1m: number;
  output_per_1m: number;
  cached_input_per_1m: number | null;
  cached_write_per_1m: number | null;
  batch_input_per_1m: number | null;
  batch_output_per_1m: number | null;
  cached_discount_pct: number;
  batch_discount_pct: number;
  volume_tier: string;
  volume_discount_pct: number;
  free_tier_input_tokens: number;
  free_tier_output_tokens: number;
  rate_limit_rpm: number;
  rate_limit_tpm: number;
  context_window: number;
  max_output_tokens: number;
  supports_vision: boolean;
  supports_streaming: boolean;
  supports_tools: boolean;
  sla_uptime_pct: number;
  effective_from: string;
  effective_to: string | null;
  status: string;
  last_updated: string;
  notes: string;
}

export const GENAI_PAYG_PRICING: GenAIPAYGPricing[] = [
  // OpenAI Models
  {
    provider: "openai",
    model: "gpt-4o",
    model_family: "gpt-4o",
    model_version: "latest",
    region: "global",
    input_per_1m: 2.50,
    output_per_1m: 10.00,
    cached_input_per_1m: 1.25,
    cached_write_per_1m: null,
    batch_input_per_1m: 1.25,
    batch_output_per_1m: 5.00,
    cached_discount_pct: 50,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 30000,
    context_window: 128000,
    max_output_tokens: 16384,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-11-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Latest GPT-4o"
  },
  {
    provider: "openai",
    model: "gpt-4o-2024-11-20",
    model_family: "gpt-4o",
    model_version: "2024-11-20",
    region: "global",
    input_per_1m: 2.50,
    output_per_1m: 10.00,
    cached_input_per_1m: 1.25,
    cached_write_per_1m: null,
    batch_input_per_1m: 1.25,
    batch_output_per_1m: 5.00,
    cached_discount_pct: 50,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 30000,
    context_window: 128000,
    max_output_tokens: 16384,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-11-20",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: ""
  },
  {
    provider: "openai",
    model: "gpt-4o-mini",
    model_family: "gpt-4o",
    model_version: "mini",
    region: "global",
    input_per_1m: 0.15,
    output_per_1m: 0.60,
    cached_input_per_1m: 0.075,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.075,
    batch_output_per_1m: 0.30,
    cached_discount_pct: 50,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 200000,
    context_window: 128000,
    max_output_tokens: 16384,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-07-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Cost effective"
  },
  {
    provider: "openai",
    model: "gpt-4-turbo",
    model_family: "gpt-4",
    model_version: "turbo",
    region: "global",
    input_per_1m: 10.00,
    output_per_1m: 30.00,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: 5.00,
    batch_output_per_1m: 15.00,
    cached_discount_pct: 0,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 30000,
    context_window: 128000,
    max_output_tokens: 4096,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-04-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: ""
  },
  {
    provider: "openai",
    model: "gpt-4",
    model_family: "gpt-4",
    model_version: "base",
    region: "global",
    input_per_1m: 30.00,
    output_per_1m: 60.00,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: 15.00,
    batch_output_per_1m: 30.00,
    cached_discount_pct: 0,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 200,
    rate_limit_tpm: 10000,
    context_window: 8192,
    max_output_tokens: 8192,
    supports_vision: false,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2023-03-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Original GPT-4"
  },
  {
    provider: "openai",
    model: "gpt-3.5-turbo",
    model_family: "gpt-3.5",
    model_version: "turbo",
    region: "global",
    input_per_1m: 0.50,
    output_per_1m: 1.50,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.25,
    batch_output_per_1m: 0.75,
    cached_discount_pct: 0,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 90000,
    context_window: 16385,
    max_output_tokens: 4096,
    supports_vision: false,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2023-03-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Legacy"
  },
  {
    provider: "openai",
    model: "o1",
    model_family: "o1",
    model_version: "latest",
    region: "global",
    input_per_1m: 15.00,
    output_per_1m: 60.00,
    cached_input_per_1m: 7.50,
    cached_write_per_1m: null,
    batch_input_per_1m: null,
    batch_output_per_1m: null,
    cached_discount_pct: 50,
    batch_discount_pct: 0,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 30000,
    context_window: 200000,
    max_output_tokens: 100000,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: false,
    sla_uptime_pct: 99.9,
    effective_from: "2024-12-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Reasoning model"
  },
  {
    provider: "openai",
    model: "o1-mini",
    model_family: "o1",
    model_version: "mini",
    region: "global",
    input_per_1m: 3.00,
    output_per_1m: 12.00,
    cached_input_per_1m: 1.50,
    cached_write_per_1m: null,
    batch_input_per_1m: null,
    batch_output_per_1m: null,
    cached_discount_pct: 50,
    batch_discount_pct: 0,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 150000,
    context_window: 128000,
    max_output_tokens: 65536,
    supports_vision: false,
    supports_streaming: true,
    supports_tools: false,
    sla_uptime_pct: 99.9,
    effective_from: "2024-09-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Fast reasoning"
  },
  {
    provider: "openai",
    model: "text-embedding-3-small",
    model_family: "embedding",
    model_version: "3-small",
    region: "global",
    input_per_1m: 0.02,
    output_per_1m: 0,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.01,
    batch_output_per_1m: null,
    cached_discount_pct: 0,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 1000000,
    context_window: 8191,
    max_output_tokens: 0,
    supports_vision: false,
    supports_streaming: false,
    supports_tools: false,
    sla_uptime_pct: 99.9,
    effective_from: "2024-01-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "1536 dimensions"
  },
  {
    provider: "openai",
    model: "text-embedding-3-large",
    model_family: "embedding",
    model_version: "3-large",
    region: "global",
    input_per_1m: 0.13,
    output_per_1m: 0,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.065,
    batch_output_per_1m: null,
    cached_discount_pct: 0,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 1000000,
    context_window: 8191,
    max_output_tokens: 0,
    supports_vision: false,
    supports_streaming: false,
    supports_tools: false,
    sla_uptime_pct: 99.9,
    effective_from: "2024-01-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "3072 dimensions"
  },
  // Anthropic Models
  {
    provider: "anthropic",
    model: "claude-3-5-sonnet-20241022",
    model_family: "claude-3.5",
    model_version: "sonnet",
    region: "global",
    input_per_1m: 3.00,
    output_per_1m: 15.00,
    cached_input_per_1m: 0.30,
    cached_write_per_1m: 3.75,
    batch_input_per_1m: 1.50,
    batch_output_per_1m: 7.50,
    cached_discount_pct: 90,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 50,
    rate_limit_tpm: 40000,
    context_window: 200000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-10-22",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Latest Sonnet"
  },
  {
    provider: "anthropic",
    model: "claude-3-5-haiku-20241022",
    model_family: "claude-3.5",
    model_version: "haiku",
    region: "global",
    input_per_1m: 0.80,
    output_per_1m: 4.00,
    cached_input_per_1m: 0.08,
    cached_write_per_1m: 1.00,
    batch_input_per_1m: 0.40,
    batch_output_per_1m: 2.00,
    cached_discount_pct: 90,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 50,
    rate_limit_tpm: 50000,
    context_window: 200000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-10-22",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Fast Haiku"
  },
  {
    provider: "anthropic",
    model: "claude-3-opus-20240229",
    model_family: "claude-3",
    model_version: "opus",
    region: "global",
    input_per_1m: 15.00,
    output_per_1m: 75.00,
    cached_input_per_1m: 1.50,
    cached_write_per_1m: 18.75,
    batch_input_per_1m: 7.50,
    batch_output_per_1m: 37.50,
    cached_discount_pct: 90,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 50,
    rate_limit_tpm: 20000,
    context_window: 200000,
    max_output_tokens: 4096,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-02-29",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Most capable"
  },
  {
    provider: "anthropic",
    model: "claude-3-sonnet-20240229",
    model_family: "claude-3",
    model_version: "sonnet",
    region: "global",
    input_per_1m: 3.00,
    output_per_1m: 15.00,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: 1.50,
    batch_output_per_1m: 7.50,
    cached_discount_pct: 0,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 50,
    rate_limit_tpm: 40000,
    context_window: 200000,
    max_output_tokens: 4096,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-02-29",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Balanced"
  },
  {
    provider: "anthropic",
    model: "claude-3-haiku-20240307",
    model_family: "claude-3",
    model_version: "haiku",
    region: "global",
    input_per_1m: 0.25,
    output_per_1m: 1.25,
    cached_input_per_1m: 0.03,
    cached_write_per_1m: 0.30,
    batch_input_per_1m: 0.125,
    batch_output_per_1m: 0.625,
    cached_discount_pct: 88,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 50,
    rate_limit_tpm: 50000,
    context_window: 200000,
    max_output_tokens: 4096,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-03-07",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Fastest"
  },
  // Gemini Models
  {
    provider: "gemini",
    model: "gemini-2.5-flash",
    model_family: "gemini-2.5",
    model_version: "flash",
    region: "global",
    input_per_1m: 0.15,
    output_per_1m: 0.60,
    cached_input_per_1m: 0.0375,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.075,
    batch_output_per_1m: 0.30,
    cached_discount_pct: 75,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 1000000,
    free_tier_output_tokens: 1000000,
    rate_limit_rpm: 1000,
    rate_limit_tpm: 4000000,
    context_window: 1000000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2025-01-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Latest Flash"
  },
  {
    provider: "gemini",
    model: "gemini-2.5-pro",
    model_family: "gemini-2.5",
    model_version: "pro",
    region: "global",
    input_per_1m: 1.25,
    output_per_1m: 10.00,
    cached_input_per_1m: 0.3125,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.625,
    batch_output_per_1m: 5.00,
    cached_discount_pct: 75,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 500000,
    free_tier_output_tokens: 500000,
    rate_limit_rpm: 360,
    rate_limit_tpm: 4000000,
    context_window: 1000000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2025-01-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Latest Pro"
  },
  {
    provider: "gemini",
    model: "gemini-2.0-flash",
    model_family: "gemini-2.0",
    model_version: "flash",
    region: "global",
    input_per_1m: 0,
    output_per_1m: 0,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: null,
    batch_output_per_1m: null,
    cached_discount_pct: 0,
    batch_discount_pct: 0,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 1500000,
    free_tier_output_tokens: 1500000,
    rate_limit_rpm: 1500,
    rate_limit_tpm: 4000000,
    context_window: 1000000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.0,
    effective_from: "2024-12-01",
    effective_to: null,
    status: "preview",
    last_updated: "2024-12-01",
    notes: "Free experimental"
  },
  {
    provider: "gemini",
    model: "gemini-1.5-flash",
    model_family: "gemini-1.5",
    model_version: "flash",
    region: "global",
    input_per_1m: 0.075,
    output_per_1m: 0.30,
    cached_input_per_1m: 0.01875,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.0375,
    batch_output_per_1m: 0.15,
    cached_discount_pct: 75,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 1000000,
    free_tier_output_tokens: 1000000,
    rate_limit_rpm: 1000,
    rate_limit_tpm: 4000000,
    context_window: 1000000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-05-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "<128K context"
  },
  {
    provider: "gemini",
    model: "gemini-1.5-pro",
    model_family: "gemini-1.5",
    model_version: "pro",
    region: "global",
    input_per_1m: 1.25,
    output_per_1m: 5.00,
    cached_input_per_1m: 0.3125,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.625,
    batch_output_per_1m: 2.50,
    cached_discount_pct: 75,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 500000,
    free_tier_output_tokens: 500000,
    rate_limit_rpm: 360,
    rate_limit_tpm: 4000000,
    context_window: 2000000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-05-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "<128K context"
  },
  // Azure OpenAI Models
  {
    provider: "azure_openai",
    model: "gpt-4o",
    model_family: "gpt-4o",
    model_version: "latest",
    region: "eastus",
    input_per_1m: 2.50,
    output_per_1m: 10.00,
    cached_input_per_1m: 1.25,
    cached_write_per_1m: null,
    batch_input_per_1m: 1.25,
    batch_output_per_1m: 5.00,
    cached_discount_pct: 50,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 30000,
    context_window: 128000,
    max_output_tokens: 16384,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.95,
    effective_from: "2024-11-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Azure East US"
  },
  {
    provider: "azure_openai",
    model: "gpt-4o",
    model_family: "gpt-4o",
    model_version: "latest",
    region: "westeurope",
    input_per_1m: 2.75,
    output_per_1m: 11.00,
    cached_input_per_1m: 1.38,
    cached_write_per_1m: null,
    batch_input_per_1m: 1.38,
    batch_output_per_1m: 5.50,
    cached_discount_pct: 50,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 30000,
    context_window: 128000,
    max_output_tokens: 16384,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.95,
    effective_from: "2024-11-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Azure West Europe +10%"
  },
  {
    provider: "azure_openai",
    model: "gpt-4o-mini",
    model_family: "gpt-4o",
    model_version: "mini",
    region: "eastus",
    input_per_1m: 0.15,
    output_per_1m: 0.60,
    cached_input_per_1m: 0.075,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.075,
    batch_output_per_1m: 0.30,
    cached_discount_pct: 50,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 200000,
    context_window: 128000,
    max_output_tokens: 16384,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.95,
    effective_from: "2024-07-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Azure East US"
  },
  {
    provider: "azure_openai",
    model: "gpt-4o-mini",
    model_family: "gpt-4o",
    model_version: "mini",
    region: "westeurope",
    input_per_1m: 0.165,
    output_per_1m: 0.66,
    cached_input_per_1m: 0.083,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.083,
    batch_output_per_1m: 0.33,
    cached_discount_pct: 50,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 500,
    rate_limit_tpm: 200000,
    context_window: 128000,
    max_output_tokens: 16384,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.95,
    effective_from: "2024-07-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Azure West Europe +10%"
  },
  // AWS Bedrock Models
  {
    provider: "aws_bedrock",
    model: "claude-3-5-sonnet-v2",
    model_family: "claude-3.5",
    model_version: "sonnet",
    region: "us-east-1",
    input_per_1m: 3.00,
    output_per_1m: 15.00,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: null,
    batch_output_per_1m: null,
    cached_discount_pct: 0,
    batch_discount_pct: 0,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 50,
    rate_limit_tpm: 40000,
    context_window: 200000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-10-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "AWS US East"
  },
  {
    provider: "aws_bedrock",
    model: "claude-3-5-sonnet-v2",
    model_family: "claude-3.5",
    model_version: "sonnet",
    region: "eu-west-1",
    input_per_1m: 3.30,
    output_per_1m: 16.50,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: null,
    batch_output_per_1m: null,
    cached_discount_pct: 0,
    batch_discount_pct: 0,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 50,
    rate_limit_tpm: 40000,
    context_window: 200000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-10-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "AWS EU West +10%"
  },
  {
    provider: "aws_bedrock",
    model: "claude-3-haiku",
    model_family: "claude-3",
    model_version: "haiku",
    region: "us-east-1",
    input_per_1m: 0.25,
    output_per_1m: 1.25,
    cached_input_per_1m: null,
    cached_write_per_1m: null,
    batch_input_per_1m: null,
    batch_output_per_1m: null,
    cached_discount_pct: 0,
    batch_discount_pct: 0,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 50,
    rate_limit_tpm: 50000,
    context_window: 200000,
    max_output_tokens: 4096,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-03-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "AWS US East"
  },
  // GCP Vertex Models
  {
    provider: "gcp_vertex",
    model: "gemini-1.5-pro",
    model_family: "gemini-1.5",
    model_version: "pro",
    region: "us-central1",
    input_per_1m: 1.25,
    output_per_1m: 5.00,
    cached_input_per_1m: 0.3125,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.625,
    batch_output_per_1m: 2.50,
    cached_discount_pct: 75,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 360,
    rate_limit_tpm: 4000000,
    context_window: 2000000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-05-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Vertex US Central"
  },
  {
    provider: "gcp_vertex",
    model: "gemini-1.5-pro",
    model_family: "gemini-1.5",
    model_version: "pro",
    region: "europe-west1",
    input_per_1m: 1.38,
    output_per_1m: 5.50,
    cached_input_per_1m: 0.34,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.69,
    batch_output_per_1m: 2.75,
    cached_discount_pct: 75,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 360,
    rate_limit_tpm: 4000000,
    context_window: 2000000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-05-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Vertex EU +10%"
  },
  {
    provider: "gcp_vertex",
    model: "gemini-1.5-flash",
    model_family: "gemini-1.5",
    model_version: "flash",
    region: "us-central1",
    input_per_1m: 0.075,
    output_per_1m: 0.30,
    cached_input_per_1m: 0.01875,
    cached_write_per_1m: null,
    batch_input_per_1m: 0.0375,
    batch_output_per_1m: 0.15,
    cached_discount_pct: 75,
    batch_discount_pct: 50,
    volume_tier: "standard",
    volume_discount_pct: 0,
    free_tier_input_tokens: 0,
    free_tier_output_tokens: 0,
    rate_limit_rpm: 1000,
    rate_limit_tpm: 4000000,
    context_window: 1000000,
    max_output_tokens: 8192,
    supports_vision: true,
    supports_streaming: true,
    supports_tools: true,
    sla_uptime_pct: 99.9,
    effective_from: "2024-05-01",
    effective_to: null,
    status: "active",
    last_updated: "2024-12-01",
    notes: "Vertex US Central"
  }
];

// Helper functions
export function getPaygPricingByProvider(provider: string): GenAIPAYGPricing[] {
  return GENAI_PAYG_PRICING.filter(p => p.provider === provider);
}

export function getPaygPricingByModel(provider: string, model: string): GenAIPAYGPricing | undefined {
  return GENAI_PAYG_PRICING.find(p => p.provider === provider && p.model === model);
}

export function getPaygPricingByRegion(provider: string, region: string): GenAIPAYGPricing[] {
  return GENAI_PAYG_PRICING.filter(p => p.provider === provider && p.region === region);
}

export function getActivePaygPricing(): GenAIPAYGPricing[] {
  return GENAI_PAYG_PRICING.filter(p => p.status === 'active');
}

export function getPaygProviders(): string[] {
  return [...new Set(GENAI_PAYG_PRICING.map(p => p.provider))];
}

export function calculateTokenCost(
  pricing: GenAIPAYGPricing,
  inputTokens: number,
  outputTokens: number,
  cachedTokens: number = 0
): number {
  const inputCost = (inputTokens / 1_000_000) * pricing.input_per_1m;
  const outputCost = (outputTokens / 1_000_000) * pricing.output_per_1m;
  const cachedCost = pricing.cached_input_per_1m
    ? (cachedTokens / 1_000_000) * pricing.cached_input_per_1m
    : 0;
  return inputCost + outputCost + cachedCost;
}
