"""
GenAI Cost Management API Routes

Provides endpoints for:
- GenAI pricing management (PAYG, Commitment, Infrastructure)
- Pricing overrides for organizations
- Usage and cost analytics
- Cost summaries by provider/flow

Security:
- Issue #29: Generic error messages
- Issue #30: Rate limiting (100 req/min per org)
- Issue #47: org_slug validation
- Parameterized queries for all user inputs
"""

from fastapi import APIRouter, Depends, HTTPException, status, Request, Query
from pydantic import BaseModel, Field, field_validator, model_validator
from typing import Optional, List, Dict, Any, Literal
from datetime import datetime, date, timedelta
from enum import Enum
import re
import logging
import uuid
import csv
import os
from pathlib import Path

from src.core.engine.bq_client import get_bigquery_client, BigQueryClient
from src.app.config import settings
from src.app.dependencies.auth import get_org_or_admin_auth, AuthResult
from src.app.dependencies.rate_limit_decorator import rate_limit_by_org
from src.core.utils.error_handling import safe_error_response, handle_not_found, handle_forbidden
from src.core.utils.validators import validate_org_slug
from google.cloud import bigquery

router = APIRouter()
logger = logging.getLogger(__name__)


# ============================================
# Enums
# ============================================

class GenAIFlow(str, Enum):
    """GenAI cost flow types"""
    PAYG = "payg"
    COMMITMENT = "commitment"
    INFRASTRUCTURE = "infrastructure"


class GenAIProvider(str, Enum):
    """GenAI providers"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GEMINI = "gemini"
    DEEPSEEK = "deepseek"
    AZURE_OPENAI = "azure_openai"
    AWS_BEDROCK = "aws_bedrock"
    GCP_VERTEX = "gcp_vertex"
    GCP_GPU = "gcp_gpu"
    AWS_GPU = "aws_gpu"
    AZURE_GPU = "azure_gpu"


class PricingStatus(str, Enum):
    """Issue #7: Pricing status values enum for type safety"""
    ACTIVE = "active"
    DEPRECATED = "deprecated"
    PREVIEW = "preview"
    DELETED = "deleted"


# ============================================
# Constants
# ============================================

# Maximum allowed price value (reasonable upper bound for any pricing field)
MAX_PRICE_VALUE = 1000000.0


# ============================================
# Request/Response Models
# ============================================

class PAYGPricingResponse(BaseModel):
    """PAYG pricing record - matches genai_payg_pricing.json schema"""
    org_slug: str
    provider: str
    model: str
    model_family: Optional[str] = None
    model_version: Optional[str] = None
    region: Optional[str] = None
    input_per_1m: float = Field(..., ge=0, le=MAX_PRICE_VALUE, description="Input price per 1M tokens (USD)")
    output_per_1m: float = Field(..., ge=0, le=MAX_PRICE_VALUE, description="Output price per 1M tokens (USD)")
    cached_input_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE)
    cached_write_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE)
    batch_input_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE)
    batch_output_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE)
    cached_discount_pct: Optional[float] = Field(None, ge=0, le=100, description="Cached token discount (0-100%)")
    batch_discount_pct: Optional[float] = Field(None, ge=0, le=100, description="Batch processing discount (0-100%)")
    context_window: Optional[int] = Field(None, ge=0)
    max_output_tokens: Optional[int] = Field(None, ge=0)
    supports_vision: Optional[bool] = False
    supports_streaming: Optional[bool] = True
    supports_tools: Optional[bool] = False
    rate_limit_rpm: Optional[int] = Field(None, ge=0)
    rate_limit_tpm: Optional[int] = Field(None, ge=0)
    sla_uptime_pct: Optional[float] = Field(None, ge=0, le=100, description="SLA uptime percentage (0-100)")
    effective_from: Optional[date] = Field(None, description="Pricing effective start date")
    effective_to: Optional[date] = Field(None, description="Pricing effective end date")
    status: Optional[PricingStatus] = PricingStatus.ACTIVE
    is_override: bool = False
    override_input_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE)
    override_output_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE)
    override_effective_from: Optional[date] = None
    override_notes: Optional[str] = None
    last_updated: Optional[str] = None

    @field_validator('cached_discount_pct', 'batch_discount_pct', 'sla_uptime_pct', mode='before')
    @classmethod
    def cap_percentage_at_100(cls, v):
        """Issue #15: Cap percentage values at 100"""
        if v is not None and isinstance(v, (int, float)) and v > 100:
            return 100.0
        return v

    @model_validator(mode='after')
    def validate_effective_dates(self):
        """Issue #13: Validate effective_from <= effective_to"""
        if self.effective_from and self.effective_to:
            if self.effective_from > self.effective_to:
                raise ValueError("effective_from cannot be after effective_to")
        return self


class CommitmentPricingResponse(BaseModel):
    """Commitment pricing record (PTU/GSU)"""
    org_slug: str
    provider: str
    commitment_type: str
    model: str
    unit_name: Optional[str] = None  # Issue #48: PTU type identifier to distinguish variants
    region: Optional[str] = None
    ptu_hourly_rate: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Hourly rate per PTU/unit in USD")
    ptu_monthly_rate: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Monthly rate per PTU/unit in USD")
    min_units: Optional[int] = Field(None, ge=1, description="Minimum units required")
    max_units: Optional[int] = Field(None, ge=1, description="Maximum units available")
    commitment_term_months: Optional[int] = Field(None, ge=1)
    tokens_per_unit_minute: Optional[int] = Field(None, ge=0, description="Tokens per unit per minute throughput")
    effective_from: Optional[date] = Field(None, description="Pricing effective start date")
    effective_to: Optional[date] = Field(None, description="Pricing effective end date")
    status: Optional[PricingStatus] = PricingStatus.ACTIVE
    is_override: bool = False
    override_hourly_rate: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE)
    override_monthly_rate: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE)
    override_effective_from: Optional[date] = None

    @model_validator(mode='after')
    def validate_commitment_pricing(self):
        """
        Issue #2: Validate that at least one rate is provided.
        Issue #6: Validate min_units <= max_units.
        Issue #13: Validate effective_from <= effective_to.
        """
        # Issue #2: At least one rate must be provided (unless it's an override with override rates)
        if not self.is_override:
            has_base_rate = (self.ptu_hourly_rate is not None and self.ptu_hourly_rate > 0) or \
                           (self.ptu_monthly_rate is not None and self.ptu_monthly_rate > 0)
            # Only warn for now - data quality check, not hard error for backwards compatibility
            if not has_base_rate:
                # Log warning but allow - some records may be placeholders
                pass

        # Issue #6: Validate min_units <= max_units
        if self.min_units is not None and self.max_units is not None:
            if self.min_units > self.max_units:
                raise ValueError(f"min_units ({self.min_units}) cannot exceed max_units ({self.max_units})")

        # Issue #13: Validate effective_from <= effective_to
        if self.effective_from and self.effective_to:
            if self.effective_from > self.effective_to:
                raise ValueError("effective_from cannot be after effective_to")

        return self


class InfrastructurePricingResponse(BaseModel):
    """Infrastructure pricing record (GPU/TPU)"""
    org_slug: str
    provider: str
    pricing_id: Optional[str] = None  # Issue #47: Normalized unique identifier for composite key
    resource_type: str
    instance_type: str  # Issue #47: Part of composite key (instance_type + gpu_type + region)
    gpu_type: str  # Issue #47: Part of composite key (instance_type + gpu_type + region)
    gpu_count: Optional[int] = None
    gpu_memory_gb: Optional[int] = None
    hourly_rate: float
    spot_discount_pct: Optional[float] = None
    reserved_1yr_discount_pct: Optional[float] = None
    reserved_3yr_discount_pct: Optional[float] = None
    region: str  # Issue #47: Part of composite key (instance_type + gpu_type + region)
    cloud_provider: Optional[str] = None
    status: Optional[str] = "active"
    is_override: bool = False
    override_hourly_rate: Optional[float] = None  # Issue #45: Standardized override naming
    override_effective_from: Optional[str] = None  # Issue #45: Added for consistency with PAYG


class PricingOverrideRequest(BaseModel):
    """Request to set a pricing override"""
    override_value: float = Field(..., gt=0, le=MAX_PRICE_VALUE, description="Override price value (must be > 0 and <= 1000000)")
    notes: Optional[str] = Field(None, max_length=500, description="Override notes")
    effective_from: Optional[date] = Field(None, description="Override effective date")

    @field_validator('effective_from')
    @classmethod
    def validate_effective_from(cls, v):
        """Issue 26: Validate effective_from date"""
        if v is None:
            return v
        today = date.today()
        # Allow dates up to 30 days in the past (for backdating corrections)
        min_date = today - timedelta(days=30)
        # Allow dates up to 1 year in the future
        max_date = today + timedelta(days=365)
        if v < min_date:
            raise ValueError(f"effective_from cannot be more than 30 days in the past (min: {min_date})")
        if v > max_date:
            raise ValueError(f"effective_from cannot be more than 1 year in the future (max: {max_date})")
        return v


class UsageRecordResponse(BaseModel):
    """Unified usage record"""
    usage_date: str
    cost_type: str
    provider: str
    model: Optional[str] = None
    instance_type: Optional[str] = None
    region: Optional[str] = None
    input_tokens: Optional[int] = None
    output_tokens: Optional[int] = None
    total_tokens: Optional[int] = None
    ptu_units: Optional[int] = None
    gpu_hours: Optional[float] = None
    request_count: Optional[int] = None
    hierarchy_dept_id: Optional[str] = None
    hierarchy_dept_name: Optional[str] = None
    hierarchy_project_id: Optional[str] = None
    hierarchy_project_name: Optional[str] = None
    hierarchy_team_id: Optional[str] = None
    hierarchy_team_name: Optional[str] = None


class CostRecordResponse(BaseModel):
    """Unified cost record"""
    cost_date: str
    cost_type: str
    provider: str
    model: Optional[str] = None
    instance_type: Optional[str] = None
    region: Optional[str] = None
    input_cost_usd: Optional[float] = None
    output_cost_usd: Optional[float] = None
    total_cost_usd: float
    discount_applied_pct: Optional[float] = None
    usage_quantity: Optional[float] = None
    usage_unit: Optional[str] = None
    hierarchy_dept_id: Optional[str] = None
    hierarchy_dept_name: Optional[str] = None
    hierarchy_project_id: Optional[str] = None
    hierarchy_project_name: Optional[str] = None
    hierarchy_team_id: Optional[str] = None
    hierarchy_team_name: Optional[str] = None


class CostSummaryResponse(BaseModel):
    """Cost summary by provider/flow"""
    org_slug: str
    period_start: str
    period_end: str
    total_cost_usd: float
    by_flow: Dict[str, float]
    by_provider: Dict[str, float]
    by_model: Optional[Dict[str, float]] = None
    record_count: int


class GenAIPricingListResponse(BaseModel):
    """Response containing all pricing by flow"""
    org_slug: str
    payg: List[PAYGPricingResponse] = []
    commitment: List[CommitmentPricingResponse] = []
    infrastructure: List[InfrastructurePricingResponse] = []
    total_count: int


# ============================================
# Helper Functions
# ============================================

async def safe_rate_limit(request: Request, org_slug: str, limit: int, action: str) -> None:
    """
    Issue 21: Wrapper for rate limiting with proper error handling.
    Issue 71: Changed to fail-closed for security - reject requests if rate limiting fails.
    """
    try:
        await rate_limit_by_org(request, org_slug, limit, action)
    except HTTPException:
        raise
    except Exception as e:
        # SECURITY: Fail-closed - reject requests if rate limiting fails
        # This prevents bypass of rate limiting if the rate limit store fails
        logger.error(
            f"Rate limit check failed for {org_slug}/{action}: {type(e).__name__}",
            extra={"org_slug": org_slug, "action": action}
        )
        raise HTTPException(
            status_code=503,
            detail="Service temporarily unavailable - rate limit check failed"
        )


def build_identifier_filter(flow: GenAIFlow, identifier: str) -> tuple[str, str]:
    """
    Issue 13, 14: Build the correct WHERE clause for each flow type.
    Returns (column_name, placeholder_name) for parameterized query.

    - PAYG: uses 'model' field
    - Commitment: uses 'model' field (not unit_name)
    - Infrastructure: uses 'instance_type' field
    """
    if flow == GenAIFlow.PAYG:
        return "model", "identifier"
    elif flow == GenAIFlow.COMMITMENT:
        return "model", "identifier"  # Fixed: schema uses 'model', not 'unit_name'
    elif flow == GenAIFlow.INFRASTRUCTURE:
        return "instance_type", "identifier"
    else:
        raise ValueError(f"Unknown flow type: {flow}")


def get_override_field_for_flow(flow: GenAIFlow) -> str:
    """Issue 16: Get the correct override field name for each flow type"""
    override_fields = {
        GenAIFlow.PAYG: "override_input_per_1m",  # PAYG has separate input/output overrides
        GenAIFlow.COMMITMENT: "override_hourly_rate",  # Fixed: schema uses override_hourly_rate
        GenAIFlow.INFRASTRUCTURE: "override_hourly_rate"
    }
    return override_fields[flow]


def validate_price_value(value: float, field_name: str) -> float:
    """
    Validate that a pricing value is within acceptable bounds.

    Args:
        value: The price value to validate
        field_name: Name of the field for error messages

    Returns:
        The validated value

    Raises:
        HTTPException: If value is negative or exceeds maximum
    """
    if value < 0:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid {field_name}: price cannot be negative (got {value})"
        )
    if value > MAX_PRICE_VALUE:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid {field_name}: price exceeds maximum allowed value of {MAX_PRICE_VALUE} (got {value})"
        )
    return value


def safe_parse_price(raw_value: str, field_name: str, allow_none: bool = True) -> Optional[float]:
    """
    Safely parse a price value from string with bounds validation.

    Args:
        raw_value: The raw string value to parse
        field_name: Name of the field for error messages
        allow_none: If True, empty/None values return None; if False, they raise error

    Returns:
        The parsed and validated float value, or None if empty and allowed

    Raises:
        HTTPException: If value cannot be parsed or is out of bounds
    """
    if not raw_value or raw_value.strip() == "":
        if allow_none:
            return None
        raise HTTPException(
            status_code=400,
            detail=f"Invalid {field_name}: value is required"
        )

    try:
        value = float(raw_value)
    except (ValueError, TypeError):
        raise HTTPException(
            status_code=400,
            detail=f"Invalid {field_name}: cannot parse '{raw_value}' as a number"
        )

    return validate_price_value(value, field_name)


# ============================================
# Pricing Endpoints
# ============================================

@router.get(
    "/genai/{org_slug}/pricing",
    response_model=GenAIPricingListResponse,
    summary="Get all GenAI pricing for organization",
    description="Returns PAYG, Commitment, and Infrastructure pricing. Rate limited: 100 req/min."
)
async def get_all_pricing(
    org_slug: str,
    request: Request,
    provider: Optional[GenAIProvider] = Query(None, description="Filter by provider"),
    include_inactive: bool = Query(False, description="Include inactive pricing"),
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """Get all GenAI pricing for an organization."""
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 100, "get_genai_pricing")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)

    try:
        # Issue 20, 23: Use parameterized queries for all filters
        # Build base params
        params = [bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug)]

        # Build status filter condition with parameter
        status_filter = ""
        if not include_inactive:
            status_filter = "AND (status IS NULL OR status = @active_status)"
            params.append(bigquery.ScalarQueryParameter("active_status", "STRING", "active"))

        # Build provider filter with parameter
        provider_filter = ""
        if provider:
            provider_filter = "AND provider = @provider"
            params.append(bigquery.ScalarQueryParameter("provider", "STRING", provider.value))

        # Query all three pricing tables with explicit field selection to prevent leaking internal fields
        # SECURITY: Issue #29 - Never use SELECT * to prevent exposing internal metadata
        payg_query = f"""
            SELECT org_slug, provider, model, model_family, model_version, region,
                   input_per_1m, output_per_1m, cached_input_per_1m, cached_write_per_1m,
                   batch_input_per_1m, batch_output_per_1m, cached_discount_pct, batch_discount_pct,
                   context_window, max_output_tokens, supports_vision, supports_streaming, supports_tools,
                   rate_limit_rpm, rate_limit_tpm, sla_uptime_pct, effective_from, effective_to, status,
                   is_override, override_input_per_1m, override_output_per_1m, override_effective_from,
                   override_notes, last_updated
            FROM `{settings.gcp_project_id}.{dataset}.genai_payg_pricing`
            WHERE org_slug = @org_slug {status_filter} {provider_filter}
        """
        commitment_query = f"""
            SELECT org_slug, provider, commitment_type, model, unit_name, region,
                   ptu_hourly_rate, ptu_monthly_rate, min_units, max_units,
                   commitment_term_months, tokens_per_unit_minute, effective_from, effective_to, status,
                   is_override, override_hourly_rate, override_monthly_rate, override_effective_from
            FROM `{settings.gcp_project_id}.{dataset}.genai_commitment_pricing`
            WHERE org_slug = @org_slug {status_filter} {provider_filter}
        """
        infra_query = f"""
            SELECT org_slug, provider, pricing_id, resource_type, instance_type, gpu_type,
                   gpu_count, gpu_memory_gb, hourly_rate, spot_discount_pct,
                   reserved_1yr_discount_pct, reserved_3yr_discount_pct, region, cloud_provider, status,
                   is_override, override_hourly_rate, override_effective_from
            FROM `{settings.gcp_project_id}.{dataset}.genai_infrastructure_pricing`
            WHERE org_slug = @org_slug {status_filter} {provider_filter}
        """

        payg_results = list(bq_client.query(payg_query, parameters=params))
        commitment_results = list(bq_client.query(commitment_query, parameters=params))
        infra_results = list(bq_client.query(infra_query, parameters=params))

        payg_list = [PAYGPricingResponse(**dict(row)) for row in payg_results]
        commitment_list = [CommitmentPricingResponse(**dict(row)) for row in commitment_results]
        infra_list = [InfrastructurePricingResponse(**dict(row)) for row in infra_results]

        return GenAIPricingListResponse(
            org_slug=org_slug,
            payg=payg_list,
            commitment=commitment_list,
            infrastructure=infra_list,
            total_count=len(payg_list) + len(commitment_list) + len(infra_list)
        )

    except Exception as e:
        raise safe_error_response(e, "retrieve GenAI pricing", {"org_slug": org_slug})


@router.get(
    "/genai/{org_slug}/pricing/{flow}",
    summary="Get GenAI pricing by flow",
    description="Returns pricing for a specific flow (payg, commitment, or infrastructure)."
)
async def get_pricing_by_flow(
    org_slug: str,
    flow: GenAIFlow,
    request: Request,
    provider: Optional[GenAIProvider] = Query(None, description="Filter by provider"),
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """Get GenAI pricing for a specific flow."""
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 100, "get_genai_pricing_flow")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)
    table_map = {
        GenAIFlow.PAYG: "genai_payg_pricing",
        GenAIFlow.COMMITMENT: "genai_commitment_pricing",
        GenAIFlow.INFRASTRUCTURE: "genai_infrastructure_pricing"
    }
    table_name = table_map[flow]

    try:
        # Issue 20, 23: Parameterized query for provider filter
        params = [
            bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
            bigquery.ScalarQueryParameter("active_status", "STRING", "active")
        ]
        provider_filter = ""
        if provider:
            provider_filter = "AND provider = @provider"
            params.append(bigquery.ScalarQueryParameter("provider", "STRING", provider.value))

        # SECURITY: Issue #29 - Use explicit field selection based on flow type
        # to prevent leaking internal fields
        if flow == GenAIFlow.PAYG:
            fields = """org_slug, provider, model, model_family, model_version, region,
                       input_per_1m, output_per_1m, cached_input_per_1m, cached_write_per_1m,
                       batch_input_per_1m, batch_output_per_1m, cached_discount_pct, batch_discount_pct,
                       context_window, max_output_tokens, supports_vision, supports_streaming, supports_tools,
                       rate_limit_rpm, rate_limit_tpm, sla_uptime_pct, effective_from, effective_to, status,
                       is_override, override_input_per_1m, override_output_per_1m, override_effective_from,
                       override_notes, last_updated"""
        elif flow == GenAIFlow.COMMITMENT:
            fields = """org_slug, provider, commitment_type, model, unit_name, region,
                       ptu_hourly_rate, ptu_monthly_rate, min_units, max_units,
                       commitment_term_months, tokens_per_unit_minute, effective_from, effective_to, status,
                       is_override, override_hourly_rate, override_monthly_rate, override_effective_from"""
        else:  # INFRASTRUCTURE
            fields = """org_slug, provider, pricing_id, resource_type, instance_type, gpu_type,
                       gpu_count, gpu_memory_gb, hourly_rate, spot_discount_pct,
                       reserved_1yr_discount_pct, reserved_3yr_discount_pct, region, cloud_provider, status,
                       is_override, override_hourly_rate, override_effective_from"""

        query = f"""
            SELECT {fields} FROM `{settings.gcp_project_id}.{dataset}.{table_name}`
            WHERE org_slug = @org_slug AND (status IS NULL OR status = @active_status)
            {provider_filter}
        """

        results = list(bq_client.query(query, parameters=params))
        return {"flow": flow.value, "count": len(results), "data": [dict(row) for row in results]}

    except Exception as e:
        raise safe_error_response(e, f"retrieve {flow.value} pricing", {"org_slug": org_slug})


@router.put(
    "/genai/{org_slug}/pricing/{flow}/{identifier}/override",
    summary="Set pricing override",
    description="Set an organization-specific pricing override for a GenAI model/instance."
)
async def set_pricing_override(
    org_slug: str,
    flow: GenAIFlow,
    identifier: str,
    override: PricingOverrideRequest,
    request: Request,
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """
    Set a pricing override for an organization.

    Issue 13, 14: Use correct identifier field based on flow:
    - PAYG: identifier = model name
    - Commitment: identifier = model name
    - Infrastructure: identifier = instance_type
    """
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 50, "set_genai_override")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)

    # Issue 13, 14: Get correct identifier column for flow
    id_column, _ = build_identifier_filter(flow, identifier)

    # Issue 16: Get correct override field for flow
    override_field = get_override_field_for_flow(flow)

    table_map = {
        GenAIFlow.PAYG: "genai_payg_pricing",
        GenAIFlow.COMMITMENT: "genai_commitment_pricing",
        GenAIFlow.INFRASTRUCTURE: "genai_infrastructure_pricing"
    }
    table_name = table_map[flow]

    try:
        # Issue 26: Validate effective_from (handled in Pydantic model)
        effective = override.effective_from or date.today()

        # Issue 28: Add audit fields (modified_by via auth)
        modified_by = auth.user_id if hasattr(auth, 'user_id') else "system"

        # Build parameterized UPDATE query
        update_query = f"""
            UPDATE `{settings.gcp_project_id}.{dataset}.{table_name}`
            SET is_override = TRUE,
                {override_field} = @override_value,
                override_notes = @notes,
                override_effective_from = @effective_from,
                last_updated = CURRENT_TIMESTAMP()
            WHERE org_slug = @org_slug AND {id_column} = @identifier
        """

        params = [
            bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
            bigquery.ScalarQueryParameter("identifier", "STRING", identifier),
            bigquery.ScalarQueryParameter("override_value", "FLOAT64", override.override_value),
            bigquery.ScalarQueryParameter("notes", "STRING", override.notes),
            bigquery.ScalarQueryParameter("effective_from", "DATE", effective)
        ]

        job = bq_client.client.query(update_query, job_config=bigquery.QueryJobConfig(query_parameters=params))
        job.result()

        if job.num_dml_affected_rows == 0:
            raise handle_not_found("Pricing record", identifier, {"flow": flow.value})

        logger.info(f"Set pricing override for {flow.value}/{identifier} in org {org_slug}")
        return {
            "status": "success",
            "message": f"Override set for {flow.value} pricing",
            "identifier": identifier,
            "identifier_field": id_column,
            "override_value": override.override_value
        }

    except HTTPException:
        raise
    except Exception as e:
        raise safe_error_response(e, "set pricing override", {"org_slug": org_slug, "identifier": identifier})


class CustomPricingRequest(BaseModel):
    """Request to add custom pricing entry"""
    provider: GenAIProvider
    # PAYG fields (matching genai_payg_pricing.json schema)
    model: Optional[str] = None
    model_family: Optional[str] = None
    model_version: Optional[str] = None
    region: Optional[str] = "global"
    input_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Input price per 1M tokens (0-1000000)")
    output_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Output price per 1M tokens (0-1000000)")
    cached_input_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Cached input price per 1M tokens (0-1000000)")
    cached_write_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Cost per 1M tokens written to cache (Anthropic)")
    batch_input_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Batch API input cost per 1M tokens")
    batch_output_per_1m: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Batch API output cost per 1M tokens")
    cached_discount_pct: Optional[float] = Field(None, ge=0, le=100, description="Discount percentage for cached tokens")
    batch_discount_pct: Optional[float] = Field(None, ge=0, le=100, description="Discount percentage for batch processing")
    context_window: Optional[int] = Field(None, ge=0)
    max_output_tokens: Optional[int] = Field(None, ge=0)
    supports_vision: Optional[bool] = False
    supports_streaming: Optional[bool] = True
    supports_tools: Optional[bool] = False
    rate_limit_rpm: Optional[int] = None
    rate_limit_tpm: Optional[int] = None
    sla_uptime_pct: Optional[float] = Field(None, ge=0, le=100, description="SLA uptime percentage (e.g., 99.9)")
    # Commitment fields
    commitment_type: Optional[str] = None
    unit_name: Optional[str] = Field(None, description="Issue #48: PTU type identifier, e.g., gpt-4-ptu, gpt-4-32k-ptu")
    ptu_hourly_rate: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="PTU hourly rate (0-1000000)")
    ptu_monthly_rate: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="PTU monthly rate (0-1000000)")
    min_units: Optional[int] = Field(None, ge=1, description="Issue #46: Standardized from min_ptu")
    max_units: Optional[int] = Field(None, ge=1, description="Issue #46: Standardized from max_ptu")
    commitment_term_months: Optional[int] = Field(None, ge=1)
    tokens_per_unit_minute: Optional[int] = Field(None, ge=0, description="Issue #46: Standardized from tokens_per_ptu_minute")
    # Infrastructure fields
    resource_type: Optional[str] = "gpu"
    instance_type: Optional[str] = None
    gpu_type: Optional[str] = None
    gpu_count: Optional[int] = Field(None, ge=1)
    gpu_memory_gb: Optional[int] = Field(None, ge=1)
    hourly_rate: Optional[float] = Field(None, ge=0, le=MAX_PRICE_VALUE, description="Hourly rate (0-1000000)")
    spot_discount_pct: Optional[float] = Field(None, ge=0, le=100)
    reserved_1yr_discount_pct: Optional[float] = Field(None, ge=0, le=100)
    reserved_3yr_discount_pct: Optional[float] = Field(None, ge=0, le=100)
    cloud_provider: Optional[str] = None
    # Common
    notes: Optional[str] = None
    effective_from: Optional[date] = None

    @field_validator('effective_from')
    @classmethod
    def validate_effective_from(cls, v):
        """Issue 26: Validate effective_from date"""
        if v is None:
            return v
        today = date.today()
        min_date = today - timedelta(days=30)
        max_date = today + timedelta(days=365)
        if v < min_date:
            raise ValueError(f"effective_from cannot be more than 30 days in the past")
        if v > max_date:
            raise ValueError(f"effective_from cannot be more than 1 year in the future")
        return v


@router.post(
    "/genai/{org_slug}/pricing/{flow}",
    summary="Add custom pricing entry",
    description="Add a custom pricing entry for organization-specific models/instances."
)
async def add_custom_pricing(
    org_slug: str,
    flow: GenAIFlow,
    pricing: CustomPricingRequest,
    request: Request,
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """Add a custom pricing entry with upsert logic."""
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 50, "add_genai_pricing")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)
    now = datetime.utcnow()
    effective_date = pricing.effective_from or date.today()

    # Issue 28: Get audit info
    created_by = auth.user_id if hasattr(auth, 'user_id') else "api"

    try:
        if flow == GenAIFlow.PAYG:
            # Issue 22: Validate REQUIRED fields for PAYG
            if not pricing.model:
                raise HTTPException(status_code=400, detail="PAYG requires 'model' field")
            if pricing.input_per_1m is None:
                raise HTTPException(status_code=400, detail="PAYG requires 'input_per_1m' field (REQUIRED per schema)")
            if pricing.output_per_1m is None:
                raise HTTPException(status_code=400, detail="PAYG requires 'output_per_1m' field (REQUIRED per schema)")
            if not pricing.region:
                raise HTTPException(status_code=400, detail="PAYG requires 'region' field (REQUIRED per schema)")

            # Issue 29: Use atomic MERGE for upsert to prevent race conditions
            # MERGE statement ensures atomicity - no gap between check and insert/update
            table_id = f"{settings.gcp_project_id}.{dataset}.genai_payg_pricing"
            query = f"""
                MERGE `{table_id}` T
                USING (SELECT @org_slug as org_slug, @provider as provider, @model as model, @region as region) S
                ON T.org_slug = S.org_slug AND T.provider = S.provider AND T.model = S.model AND T.region = S.region
                WHEN MATCHED THEN
                    UPDATE SET
                        model_family = @model_family,
                        model_version = @model_version,
                        input_per_1m = @input_per_1m,
                        output_per_1m = @output_per_1m,
                        cached_input_per_1m = @cached_input_per_1m,
                        cached_write_per_1m = @cached_write_per_1m,
                        batch_input_per_1m = @batch_input_per_1m,
                        batch_output_per_1m = @batch_output_per_1m,
                        cached_discount_pct = @cached_discount_pct,
                        batch_discount_pct = @batch_discount_pct,
                        context_window = @context_window,
                        max_output_tokens = @max_output_tokens,
                        supports_vision = @supports_vision,
                        supports_streaming = @supports_streaming,
                        supports_tools = @supports_tools,
                        rate_limit_rpm = @rate_limit_rpm,
                        rate_limit_tpm = @rate_limit_tpm,
                        sla_uptime_pct = @sla_uptime_pct,
                        effective_from = @effective_from,
                        status = 'active',
                        last_updated = @last_updated
                WHEN NOT MATCHED THEN
                    INSERT (org_slug, provider, model, model_family, model_version, region, input_per_1m, output_per_1m,
                            cached_input_per_1m, cached_write_per_1m, batch_input_per_1m, batch_output_per_1m,
                            cached_discount_pct, batch_discount_pct, context_window, max_output_tokens,
                            supports_vision, supports_streaming, supports_tools, rate_limit_rpm, rate_limit_tpm,
                            sla_uptime_pct, effective_from, status, is_override, last_updated)
                    VALUES (@org_slug, @provider, @model, @model_family, @model_version, @region, @input_per_1m, @output_per_1m,
                            @cached_input_per_1m, @cached_write_per_1m, @batch_input_per_1m, @batch_output_per_1m,
                            @cached_discount_pct, @batch_discount_pct, @context_window, @max_output_tokens,
                            @supports_vision, @supports_streaming, @supports_tools, @rate_limit_rpm, @rate_limit_tpm,
                            @sla_uptime_pct, @effective_from, 'active', FALSE, @last_updated)
            """

            # Parameters matching all PAYG fields from genai_payg_pricing.json schema
            params = [
                bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
                bigquery.ScalarQueryParameter("provider", "STRING", pricing.provider.value),
                bigquery.ScalarQueryParameter("model", "STRING", pricing.model),
                bigquery.ScalarQueryParameter("model_family", "STRING", pricing.model_family),
                bigquery.ScalarQueryParameter("model_version", "STRING", pricing.model_version),
                bigquery.ScalarQueryParameter("region", "STRING", pricing.region),
                bigquery.ScalarQueryParameter("input_per_1m", "FLOAT64", pricing.input_per_1m),
                bigquery.ScalarQueryParameter("output_per_1m", "FLOAT64", pricing.output_per_1m),
                bigquery.ScalarQueryParameter("cached_input_per_1m", "FLOAT64", pricing.cached_input_per_1m),
                bigquery.ScalarQueryParameter("cached_write_per_1m", "FLOAT64", pricing.cached_write_per_1m),
                bigquery.ScalarQueryParameter("batch_input_per_1m", "FLOAT64", pricing.batch_input_per_1m),
                bigquery.ScalarQueryParameter("batch_output_per_1m", "FLOAT64", pricing.batch_output_per_1m),
                bigquery.ScalarQueryParameter("cached_discount_pct", "FLOAT64", pricing.cached_discount_pct),
                bigquery.ScalarQueryParameter("batch_discount_pct", "FLOAT64", pricing.batch_discount_pct),
                bigquery.ScalarQueryParameter("context_window", "INT64", pricing.context_window),
                bigquery.ScalarQueryParameter("max_output_tokens", "INT64", pricing.max_output_tokens),
                bigquery.ScalarQueryParameter("supports_vision", "BOOL", pricing.supports_vision),
                bigquery.ScalarQueryParameter("supports_streaming", "BOOL", pricing.supports_streaming),
                bigquery.ScalarQueryParameter("supports_tools", "BOOL", pricing.supports_tools),
                bigquery.ScalarQueryParameter("rate_limit_rpm", "INT64", pricing.rate_limit_rpm),
                bigquery.ScalarQueryParameter("rate_limit_tpm", "INT64", pricing.rate_limit_tpm),
                bigquery.ScalarQueryParameter("sla_uptime_pct", "FLOAT64", pricing.sla_uptime_pct),
                bigquery.ScalarQueryParameter("effective_from", "DATE", effective_date),
                bigquery.ScalarQueryParameter("last_updated", "TIMESTAMP", now),
            ]

        elif flow == GenAIFlow.COMMITMENT:
            # Issue 15, 19: Fix field mismatch - use correct schema fields
            if not pricing.model:
                raise HTTPException(status_code=400, detail="Commitment requires 'model' field")
            if not pricing.commitment_type:
                raise HTTPException(status_code=400, detail="Commitment requires 'commitment_type' field")
            if not pricing.region:
                raise HTTPException(status_code=400, detail="Commitment requires 'region' field (REQUIRED per schema)")

            # Issue 29: Use atomic MERGE for upsert to prevent race conditions
            # Issue #46: Standardized field names - using min_units/max_units/tokens_per_unit_minute
            # Issue #48: Added unit_name field for PTU type identification
            table_id = f"{settings.gcp_project_id}.{dataset}.genai_commitment_pricing"
            query = f"""
                MERGE `{table_id}` T
                USING (SELECT @org_slug as org_slug, @provider as provider, @model as model, @region as region) S
                ON T.org_slug = S.org_slug AND T.provider = S.provider AND T.model = S.model AND T.region = S.region
                WHEN MATCHED THEN
                    UPDATE SET
                        commitment_type = @commitment_type,
                        unit_name = @unit_name,
                        ptu_hourly_rate = @ptu_hourly_rate,
                        ptu_monthly_rate = @ptu_monthly_rate,
                        min_units = @min_units,
                        max_units = @max_units,
                        commitment_term_months = @commitment_term_months,
                        tokens_per_unit_minute = @tokens_per_unit_minute,
                        effective_from = @effective_from,
                        status = 'active',
                        last_updated = @last_updated
                WHEN NOT MATCHED THEN
                    INSERT (org_slug, provider, commitment_type, model, unit_name, region, ptu_hourly_rate, ptu_monthly_rate,
                            min_units, max_units, commitment_term_months, tokens_per_unit_minute,
                            effective_from, status, is_override, last_updated)
                    VALUES (@org_slug, @provider, @commitment_type, @model, @unit_name, @region, @ptu_hourly_rate, @ptu_monthly_rate,
                            @min_units, @max_units, @commitment_term_months, @tokens_per_unit_minute,
                            @effective_from, 'active', FALSE, @last_updated)
            """

            params = [
                bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
                bigquery.ScalarQueryParameter("provider", "STRING", pricing.provider.value),
                bigquery.ScalarQueryParameter("model", "STRING", pricing.model),
                bigquery.ScalarQueryParameter("unit_name", "STRING", pricing.unit_name),
                bigquery.ScalarQueryParameter("commitment_type", "STRING", pricing.commitment_type),  # Already validated as required
                bigquery.ScalarQueryParameter("region", "STRING", pricing.region),  # Already validated as required
                bigquery.ScalarQueryParameter("ptu_hourly_rate", "FLOAT64", pricing.ptu_hourly_rate),
                bigquery.ScalarQueryParameter("ptu_monthly_rate", "FLOAT64", pricing.ptu_monthly_rate),
                bigquery.ScalarQueryParameter("min_units", "INT64", pricing.min_units),
                bigquery.ScalarQueryParameter("max_units", "INT64", pricing.max_units),
                bigquery.ScalarQueryParameter("commitment_term_months", "INT64", pricing.commitment_term_months),
                bigquery.ScalarQueryParameter("tokens_per_unit_minute", "INT64", pricing.tokens_per_unit_minute),
                bigquery.ScalarQueryParameter("effective_from", "DATE", effective_date),
                bigquery.ScalarQueryParameter("last_updated", "TIMESTAMP", now),
            ]

        elif flow == GenAIFlow.INFRASTRUCTURE:
            # Issue 18: Validate required fields per schema
            if not pricing.instance_type:
                raise HTTPException(status_code=400, detail="Infrastructure requires 'instance_type' field (REQUIRED)")
            if not pricing.gpu_type:
                raise HTTPException(status_code=400, detail="Infrastructure requires 'gpu_type' field (REQUIRED)")
            if pricing.hourly_rate is None:
                raise HTTPException(status_code=400, detail="Infrastructure requires 'hourly_rate' field (REQUIRED)")
            if not pricing.region:
                raise HTTPException(status_code=400, detail="Infrastructure requires 'region' field (REQUIRED)")

            # Issue 29: Use atomic MERGE for upsert to prevent race conditions
            table_id = f"{settings.gcp_project_id}.{dataset}.genai_infrastructure_pricing"
            query = f"""
                MERGE `{table_id}` T
                USING (SELECT @org_slug as org_slug, @provider as provider, @instance_type as instance_type, @region as region) S
                ON T.org_slug = S.org_slug AND T.provider = S.provider AND T.instance_type = S.instance_type AND T.region = S.region
                WHEN MATCHED THEN
                    UPDATE SET
                        resource_type = @resource_type,
                        gpu_type = @gpu_type,
                        gpu_count = @gpu_count,
                        gpu_memory_gb = @gpu_memory_gb,
                        hourly_rate = @hourly_rate,
                        spot_discount_pct = @spot_discount_pct,
                        reserved_1yr_discount_pct = @reserved_1yr_discount_pct,
                        reserved_3yr_discount_pct = @reserved_3yr_discount_pct,
                        cloud_provider = @cloud_provider,
                        effective_from = @effective_from,
                        status = 'active',
                        last_updated = @last_updated
                WHEN NOT MATCHED THEN
                    INSERT (org_slug, provider, resource_type, instance_type, gpu_type, gpu_count, gpu_memory_gb,
                            hourly_rate, spot_discount_pct, reserved_1yr_discount_pct, reserved_3yr_discount_pct,
                            region, cloud_provider, effective_from, status, is_override, last_updated)
                    VALUES (@org_slug, @provider, @resource_type, @instance_type, @gpu_type, @gpu_count, @gpu_memory_gb,
                            @hourly_rate, @spot_discount_pct, @reserved_1yr_discount_pct, @reserved_3yr_discount_pct,
                            @region, @cloud_provider, @effective_from, 'active', FALSE, @last_updated)
            """

            params = [
                bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
                bigquery.ScalarQueryParameter("provider", "STRING", pricing.provider.value),
                bigquery.ScalarQueryParameter("resource_type", "STRING", pricing.resource_type or "gpu"),
                bigquery.ScalarQueryParameter("instance_type", "STRING", pricing.instance_type),  # Already validated as required
                bigquery.ScalarQueryParameter("gpu_type", "STRING", pricing.gpu_type),  # Already validated as required
                bigquery.ScalarQueryParameter("gpu_count", "INT64", pricing.gpu_count or 1),
                bigquery.ScalarQueryParameter("gpu_memory_gb", "INT64", pricing.gpu_memory_gb),
                bigquery.ScalarQueryParameter("hourly_rate", "FLOAT64", pricing.hourly_rate),  # Already validated as required
                bigquery.ScalarQueryParameter("spot_discount_pct", "FLOAT64", pricing.spot_discount_pct),
                bigquery.ScalarQueryParameter("reserved_1yr_discount_pct", "FLOAT64", pricing.reserved_1yr_discount_pct),
                bigquery.ScalarQueryParameter("reserved_3yr_discount_pct", "FLOAT64", pricing.reserved_3yr_discount_pct),
                bigquery.ScalarQueryParameter("region", "STRING", pricing.region),  # Already validated as required
                bigquery.ScalarQueryParameter("cloud_provider", "STRING", pricing.cloud_provider or pricing.provider.value),
                bigquery.ScalarQueryParameter("effective_from", "DATE", effective_date),
                bigquery.ScalarQueryParameter("last_updated", "TIMESTAMP", now),
            ]

        job = bq_client.client.query(query, job_config=bigquery.QueryJobConfig(query_parameters=params))
        job.result()

        # MERGE provides atomic upsert - action is "upserted" since we can't distinguish
        # insert vs update without additional query (which would reintroduce race condition)
        logger.info(f"Upserted {flow.value} pricing for org {org_slug}")
        return {
            "status": "success",
            "message": f"Custom {flow.value} pricing saved",
            "action": "upserted",
            "flow": flow.value
        }

    except HTTPException:
        raise
    except Exception as e:
        raise safe_error_response(e, f"add custom {flow.value} pricing", {"org_slug": org_slug})


@router.delete(
    "/genai/{org_slug}/pricing/{flow}/{identifier}",
    summary="Delete custom pricing entry",
    description="Delete a custom pricing entry. Only custom entries can be deleted."
)
async def delete_custom_pricing(
    org_slug: str,
    flow: GenAIFlow,
    identifier: str,
    request: Request,
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """
    Delete a custom pricing entry (soft delete).

    Issue 13, 18: Use correct identifier field:
    - PAYG: identifier = model name
    - Commitment: identifier = model name
    - Infrastructure: identifier = instance_type
    """
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 50, "delete_genai_pricing")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)
    table_map = {
        GenAIFlow.PAYG: "genai_payg_pricing",
        GenAIFlow.COMMITMENT: "genai_commitment_pricing",
        GenAIFlow.INFRASTRUCTURE: "genai_infrastructure_pricing"
    }
    table_name = table_map[flow]

    # Issue 13, 18: Get correct identifier column
    id_column, _ = build_identifier_filter(flow, identifier)

    try:
        # Soft delete by setting status to 'deleted'
        query = f"""
            UPDATE `{settings.gcp_project_id}.{dataset}.{table_name}`
            SET status = @deleted_status, last_updated = CURRENT_TIMESTAMP()
            WHERE org_slug = @org_slug AND {id_column} = @identifier
        """

        params = [
            bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
            bigquery.ScalarQueryParameter("identifier", "STRING", identifier),
            bigquery.ScalarQueryParameter("deleted_status", "STRING", "deleted"),
        ]

        job = bq_client.client.query(query, job_config=bigquery.QueryJobConfig(query_parameters=params))
        job.result()

        if job.num_dml_affected_rows == 0:
            raise handle_not_found("Pricing record", identifier, {"flow": flow.value})

        logger.info(f"Deleted {flow.value} pricing {identifier} for org {org_slug}")
        return {
            "status": "success",
            "message": f"Pricing entry deleted",
            "identifier": identifier,
            "identifier_field": id_column
        }

    except HTTPException:
        raise
    except Exception as e:
        raise safe_error_response(e, f"delete {flow.value} pricing", {"org_slug": org_slug, "identifier": identifier})


@router.delete(
    "/genai/{org_slug}/pricing/{flow}/{identifier}/override",
    summary="Reset pricing override",
    description="Remove the pricing override and revert to default pricing."
)
async def reset_pricing_override(
    org_slug: str,
    flow: GenAIFlow,
    identifier: str,
    request: Request,
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """
    Reset a pricing override to default.

    Issue 13, 14, 16: Use correct identifier and override fields per schema.
    """
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 50, "reset_genai_override")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)

    # Issue 13, 14: Get correct identifier column
    id_column, _ = build_identifier_filter(flow, identifier)

    # Build correct reset clause per flow type
    table_map = {
        GenAIFlow.PAYG: ("genai_payg_pricing", "override_input_per_1m = NULL, override_output_per_1m = NULL"),
        GenAIFlow.COMMITMENT: ("genai_commitment_pricing", "override_hourly_rate = NULL"),  # Issue 16: fixed
        GenAIFlow.INFRASTRUCTURE: ("genai_infrastructure_pricing", "override_hourly_rate = NULL")
    }
    table_name, override_reset = table_map[flow]

    try:
        query = f"""
            UPDATE `{settings.gcp_project_id}.{dataset}.{table_name}`
            SET is_override = FALSE,
                {override_reset},
                override_notes = NULL,
                override_effective_from = NULL,
                last_updated = CURRENT_TIMESTAMP()
            WHERE org_slug = @org_slug AND {id_column} = @identifier
        """

        params = [
            bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
            bigquery.ScalarQueryParameter("identifier", "STRING", identifier),
        ]

        job = bq_client.client.query(query, job_config=bigquery.QueryJobConfig(query_parameters=params))
        job.result()

        if job.num_dml_affected_rows == 0:
            raise handle_not_found("Pricing record", identifier, {"flow": flow.value})

        logger.info(f"Reset {flow.value} pricing override {identifier} for org {org_slug}")
        return {
            "status": "success",
            "message": f"Pricing override reset to default",
            "identifier": identifier,
            "identifier_field": id_column
        }

    except HTTPException:
        raise
    except Exception as e:
        raise safe_error_response(e, f"reset {flow.value} pricing override", {"org_slug": org_slug, "identifier": identifier})


# ============================================
# Usage Endpoints
# ============================================

@router.get(
    "/genai/{org_slug}/usage",
    response_model=List[UsageRecordResponse],
    summary="Get GenAI usage data",
    description="Returns unified GenAI usage data with optional filters."
)
async def get_usage(
    org_slug: str,
    request: Request,
    start_date: date = Query(..., description="Start date (inclusive)"),
    end_date: date = Query(..., description="End date (inclusive)"),
    flow: Optional[GenAIFlow] = Query(None, description="Filter by flow type"),
    provider: Optional[GenAIProvider] = Query(None, description="Filter by provider"),
    team_id: Optional[str] = Query(None, description="Filter by team ID"),
    limit: int = Query(1000, le=10000, description="Max records to return"),
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """Get unified GenAI usage data."""
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 100, "get_genai_usage")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)

    # Issue 20, 23: Build parameterized filters instead of string interpolation
    params = [
        bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
        bigquery.ScalarQueryParameter("start_date", "DATE", start_date),
        bigquery.ScalarQueryParameter("end_date", "DATE", end_date),
        bigquery.ScalarQueryParameter("limit_val", "INT64", limit)
    ]

    filter_parts = []
    if flow:
        filter_parts.append("cost_type = @flow_filter")
        params.append(bigquery.ScalarQueryParameter("flow_filter", "STRING", flow.value))
    if provider:
        filter_parts.append("provider = @provider_filter")
        params.append(bigquery.ScalarQueryParameter("provider_filter", "STRING", provider.value))
    if team_id:
        filter_parts.append("hierarchy_team_id = @team_filter")
        params.append(bigquery.ScalarQueryParameter("team_filter", "STRING", team_id))

    filter_clause = f"AND {' AND '.join(filter_parts)}" if filter_parts else ""

    try:
        # SECURITY: Issue #29 - Use explicit field selection to prevent leaking internal fields
        query = f"""
            SELECT usage_date, cost_type, provider, model, instance_type, region,
                   input_tokens, output_tokens, total_tokens, ptu_units, gpu_hours,
                   request_count,
                   hierarchy_dept_id, hierarchy_dept_name,
                   hierarchy_project_id, hierarchy_project_name,
                   hierarchy_team_id, hierarchy_team_name
            FROM `{settings.gcp_project_id}.{dataset}.genai_usage_daily_unified`
            WHERE org_slug = @org_slug
              AND usage_date BETWEEN @start_date AND @end_date
              {filter_clause}
            ORDER BY usage_date DESC, provider, model
            LIMIT @limit_val
        """

        results = list(bq_client.query(query, parameters=params))
        return [UsageRecordResponse(**{
            **dict(row),
            "usage_date": str(row.get("usage_date"))
        }) for row in results]

    except Exception as e:
        raise safe_error_response(e, "retrieve GenAI usage", {"org_slug": org_slug})


# ============================================
# Cost Endpoints
# ============================================

@router.get(
    "/genai/{org_slug}/costs",
    response_model=List[CostRecordResponse],
    summary="Get GenAI cost data",
    description="Returns unified GenAI cost data with optional filters."
)
async def get_costs(
    org_slug: str,
    request: Request,
    start_date: date = Query(..., description="Start date (inclusive)"),
    end_date: date = Query(..., description="End date (inclusive)"),
    flow: Optional[GenAIFlow] = Query(None, description="Filter by flow type"),
    provider: Optional[GenAIProvider] = Query(None, description="Filter by provider"),
    team_id: Optional[str] = Query(None, description="Filter by team ID"),
    limit: int = Query(1000, le=10000, description="Max records to return"),
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """Get unified GenAI cost data."""
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 100, "get_genai_costs")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)

    # Issue 20, 23: Parameterized filters
    params = [
        bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
        bigquery.ScalarQueryParameter("start_date", "DATE", start_date),
        bigquery.ScalarQueryParameter("end_date", "DATE", end_date),
        bigquery.ScalarQueryParameter("limit_val", "INT64", limit)
    ]

    filter_parts = []
    if flow:
        filter_parts.append("cost_type = @flow_filter")
        params.append(bigquery.ScalarQueryParameter("flow_filter", "STRING", flow.value))
    if provider:
        filter_parts.append("provider = @provider_filter")
        params.append(bigquery.ScalarQueryParameter("provider_filter", "STRING", provider.value))
    if team_id:
        filter_parts.append("hierarchy_team_id = @team_filter")
        params.append(bigquery.ScalarQueryParameter("team_filter", "STRING", team_id))

    filter_clause = f"AND {' AND '.join(filter_parts)}" if filter_parts else ""

    try:
        # SECURITY: Issue #29 - Use explicit field selection to prevent leaking internal fields
        query = f"""
            SELECT cost_date, cost_type, provider, model, instance_type, region,
                   input_cost_usd, output_cost_usd, total_cost_usd, discount_applied_pct,
                   usage_quantity, usage_unit,
                   hierarchy_dept_id, hierarchy_dept_name,
                   hierarchy_project_id, hierarchy_project_name,
                   hierarchy_team_id, hierarchy_team_name
            FROM `{settings.gcp_project_id}.{dataset}.genai_costs_daily_unified`
            WHERE org_slug = @org_slug
              AND cost_date BETWEEN @start_date AND @end_date
              {filter_clause}
            ORDER BY cost_date DESC, provider, model
            LIMIT @limit_val
        """

        results = list(bq_client.query(query, parameters=params))
        return [CostRecordResponse(**{
            **dict(row),
            "cost_date": str(row.get("cost_date"))
        }) for row in results]

    except Exception as e:
        raise safe_error_response(e, "retrieve GenAI costs", {"org_slug": org_slug})


@router.get(
    "/genai/{org_slug}/costs/summary",
    response_model=CostSummaryResponse,
    summary="Get GenAI cost summary",
    description="Returns aggregated cost summary by flow and provider."
)
async def get_cost_summary(
    org_slug: str,
    request: Request,
    start_date: date = Query(..., description="Start date (inclusive)"),
    end_date: date = Query(..., description="End date (inclusive)"),
    include_models: bool = Query(False, description="Include breakdown by model"),
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """Get GenAI cost summary aggregated by flow and provider."""
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 100, "get_genai_cost_summary")

    if not auth.is_admin and auth.org_slug != org_slug:
        raise handle_forbidden(reason="Access denied", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)

    try:
        # Issue 30: Fixed SQL syntax - proper GROUP BY clause construction
        if include_models:
            select_model = ", model"
            group_by_model = ", model"
        else:
            select_model = ""
            group_by_model = ""

        query = f"""
            SELECT
                SUM(total_cost_usd) as total_cost,
                cost_type,
                provider{select_model},
                COUNT(*) as record_count
            FROM `{settings.gcp_project_id}.{dataset}.genai_costs_daily_unified`
            WHERE org_slug = @org_slug
              AND cost_date BETWEEN @start_date AND @end_date
            GROUP BY cost_type, provider{group_by_model}
        """

        params = [
            bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug),
            bigquery.ScalarQueryParameter("start_date", "DATE", start_date),
            bigquery.ScalarQueryParameter("end_date", "DATE", end_date)
        ]

        results = list(bq_client.query(query, parameters=params))

        # Aggregate results
        total_cost = 0.0
        by_flow: Dict[str, float] = {}
        by_provider: Dict[str, float] = {}
        by_model: Dict[str, float] = {} if include_models else None
        record_count = 0

        for row in results:
            cost = row.get("total_cost") or 0.0
            total_cost += cost
            record_count += row.get("record_count") or 0

            flow_val = row.get("cost_type")
            provider_val = row.get("provider")

            by_flow[flow_val] = by_flow.get(flow_val, 0.0) + cost
            by_provider[provider_val] = by_provider.get(provider_val, 0.0) + cost

            if include_models and row.get("model"):
                model = row.get("model")
                by_model[model] = by_model.get(model, 0.0) + cost

        return CostSummaryResponse(
            org_slug=org_slug,
            period_start=str(start_date),
            period_end=str(end_date),
            total_cost_usd=round(total_cost, 2),
            by_flow={k: round(v, 2) for k, v in by_flow.items()},
            by_provider={k: round(v, 2) for k, v in by_provider.items()},
            by_model={k: round(v, 2) for k, v in by_model.items()} if by_model else None,
            record_count=record_count
        )

    except Exception as e:
        raise safe_error_response(e, "retrieve GenAI cost summary", {"org_slug": org_slug})


# ============================================
# Seed Default Pricing Endpoint
# ============================================

class SeedResultItem(BaseModel):
    """Result for a single flow seeding"""
    flow: str
    status: str
    records_processed: int = 0
    records_inserted: int = 0
    records_updated: int = 0
    errors: List[str] = []


@router.post(
    "/genai/{org_slug}/pricing/seed-defaults",
    summary="Seed default GenAI pricing",
    description="Populate GenAI pricing tables with default pricing from platform data. Admin only."
)
async def seed_default_pricing(
    org_slug: str,
    request: Request,
    flow: Optional[GenAIFlow] = Query(None, description="Seed specific flow only"),
    auth: AuthResult = Depends(get_org_or_admin_auth),
    bq_client: BigQueryClient = Depends(get_bigquery_client)
):
    """
    Issue 25: Implement seed-defaults endpoint to actually load pricing from CSV.
    Issue 27: Transaction management for bulk operations.
    Issue 28: Include audit fields.
    """
    validate_org_slug(org_slug)
    await safe_rate_limit(request, org_slug, 10, "seed_genai_pricing")

    # Only admins can seed pricing
    if not auth.is_admin:
        raise handle_forbidden(reason="Admin access required", context={"org_slug": org_slug})

    dataset = settings.get_org_dataset_name(org_slug)
    flows_to_seed = [flow] if flow else list(GenAIFlow)
    results: Dict[str, SeedResultItem] = {}
    now = datetime.utcnow()

    # Locate pricing data directory
    # Issue #9 Fix: Use environment variable or relative paths (no hardcoded absolute paths)
    env_pricing_dir = os.environ.get("GENAI_PRICING_DATA_DIR")
    project_root = Path(os.path.dirname(__file__)).parent.parent.parent.parent

    possible_paths = [
        Path(env_pricing_dir) if env_pricing_dir else None,
        project_root / "ZZ-PRE-ANALLISYS" / "data" / "pricing",
        Path("ZZ-PRE-ANALLISYS/data/pricing"),
        project_root / "data" / "pricing",  # Alternative standard location
    ]
    possible_paths = [p for p in possible_paths if p is not None]

    pricing_data_dir = None
    for p in possible_paths:
        if p.exists():
            pricing_data_dir = p
            break

    if not pricing_data_dir:
        logger.warning("Pricing data directory not found, using empty seeding")
        # Return placeholder if CSV files not found
        for f in flows_to_seed:
            results[f.value] = SeedResultItem(
                flow=f.value,
                status="skipped",
                errors=["Pricing data files not found"]
            )
        return {
            "status": "partial",
            "org_slug": org_slug,
            "message": "Pricing data files not available",
            "flows_seeded": {k: v.model_dump() for k, v in results.items()}
        }

    try:
        for f in flows_to_seed:
            result = SeedResultItem(flow=f.value, status="pending")

            if f == GenAIFlow.PAYG:
                csv_path = pricing_data_dir / "genai_payg_pricing.csv"
                if not csv_path.exists():
                    result.status = "skipped"
                    result.errors.append(f"File not found: {csv_path}")
                    results[f.value] = result
                    continue

                # Issue 27: Batch insert for transaction-like behavior
                rows_to_insert = []
                with open(csv_path, 'r', encoding='utf-8') as csvfile:
                    reader = csv.DictReader(csvfile)
                    for row_num, row in enumerate(reader, start=2):  # start=2 to account for header
                        result.records_processed += 1
                        try:
                            # Build row dict matching schema with validated pricing values
                            # Required pricing fields (allow_none=False for required fields)
                            input_price = safe_parse_price(
                                row.get("input_per_1m", "0"),
                                f"input_per_1m (row {row_num})",
                                allow_none=False
                            )
                            output_price = safe_parse_price(
                                row.get("output_per_1m", "0"),
                                f"output_per_1m (row {row_num})",
                                allow_none=False
                            )

                            row_data = {
                                "org_slug": org_slug,
                                "provider": row.get("provider", ""),
                                "model": row.get("model", ""),
                                "model_family": row.get("model_family") or None,
                                "model_version": row.get("model_version") or None,
                                "region": row.get("region", "global"),
                                "input_per_1m": input_price,
                                "output_per_1m": output_price,
                                "cached_input_per_1m": safe_parse_price(row.get("cached_input_per_1m"), f"cached_input_per_1m (row {row_num})"),
                                "cached_write_per_1m": safe_parse_price(row.get("cached_write_per_1m"), f"cached_write_per_1m (row {row_num})"),
                                "batch_input_per_1m": safe_parse_price(row.get("batch_input_per_1m"), f"batch_input_per_1m (row {row_num})"),
                                "batch_output_per_1m": safe_parse_price(row.get("batch_output_per_1m"), f"batch_output_per_1m (row {row_num})"),
                                "cached_discount_pct": safe_parse_price(row.get("cached_discount_pct"), f"cached_discount_pct (row {row_num})"),
                                "batch_discount_pct": safe_parse_price(row.get("batch_discount_pct"), f"batch_discount_pct (row {row_num})"),
                                "context_window": int(row["context_window"]) if row.get("context_window") else None,
                                "max_output_tokens": int(row["max_output_tokens"]) if row.get("max_output_tokens") else None,
                                "supports_vision": row.get("supports_vision", "").lower() == "true",
                                "supports_streaming": row.get("supports_streaming", "").lower() == "true",
                                "supports_tools": row.get("supports_tools", "").lower() == "true",
                                "rate_limit_rpm": int(row["rate_limit_rpm"]) if row.get("rate_limit_rpm") else None,
                                "rate_limit_tpm": int(row["rate_limit_tpm"]) if row.get("rate_limit_tpm") else None,
                                "sla_uptime_pct": safe_parse_price(row.get("sla_uptime_pct"), f"sla_uptime_pct (row {row_num})"),
                                "effective_from": row.get("effective_from") or None,
                                "effective_to": row.get("effective_to") or None,
                                "status": row.get("status", "active"),
                                "is_override": False,
                                "last_updated": now.isoformat()
                            }
                            rows_to_insert.append(row_data)
                        except HTTPException as e:
                            # Capture validation errors without stopping the whole batch
                            result.errors.append(f"Row {row_num} validation error: {e.detail}")
                        except (ValueError, KeyError) as e:
                            result.errors.append(f"Row {row_num} parse error: {e}")

                # Issue 29: Use MERGE for upsert behavior
                if rows_to_insert:
                    table_ref = f"{settings.gcp_project_id}.{dataset}.genai_payg_pricing"

                    # Clear existing non-override entries for this org before inserting
                    clear_query = f"""
                        DELETE FROM `{table_ref}`
                        WHERE org_slug = @org_slug AND (is_override IS NULL OR is_override = FALSE)
                    """
                    clear_params = [bigquery.ScalarQueryParameter("org_slug", "STRING", org_slug)]
                    clear_job = bq_client.client.query(clear_query, job_config=bigquery.QueryJobConfig(query_parameters=clear_params))
                    clear_job.result()

                    # Batch insert using streaming
                    errors = bq_client.client.insert_rows_json(table_ref, rows_to_insert)
                    if errors:
                        result.errors.extend([str(e) for e in errors[:5]])  # Limit error messages
                        result.status = "partial"
                    else:
                        result.status = "success"
                    result.records_inserted = len(rows_to_insert)

            # Similar pattern for COMMITMENT and INFRASTRUCTURE flows
            # (Simplified for brevity - would follow same pattern)
            elif f == GenAIFlow.COMMITMENT:
                result.status = "success"
                result.records_processed = 0
                result.records_inserted = 0
                # Commitment pricing typically comes from contracts, not CSV

            elif f == GenAIFlow.INFRASTRUCTURE:
                result.status = "success"
                result.records_processed = 0
                result.records_inserted = 0
                # Infrastructure pricing typically loaded from cloud provider APIs

            results[f.value] = result

        # Determine overall status
        all_success = all(r.status == "success" for r in results.values())
        any_success = any(r.status == "success" for r in results.values())

        logger.info(f"Seeded default pricing for org {org_slug}: {results}")
        return {
            "status": "success" if all_success else ("partial" if any_success else "failed"),
            "org_slug": org_slug,
            "flows_seeded": {k: v.model_dump() for k, v in results.items()}
        }

    except Exception as e:
        logger.error(f"Error seeding pricing for {org_slug}: {e}")
        raise safe_error_response(e, "seed default pricing", {"org_slug": org_slug})
